HloModule add_one_shard_parallel, is_scheduled=true, entry_computation_layout={(f32[16,64]{1,0})->(f32[16,64]{1,0})}, allow_spmd_sharding_propagation_to_output={true}

%fused_computation (param_1: f32[16,64]) -> f32[16,64] {
  %param_1 = f32[16,64]{1,0} parameter(0)
  %constant_1 = f32[] constant(1)
  %broadcast.2 = f32[16,64]{1,0} broadcast(f32[] %constant_1), dimensions={}, metadata={op_name="parallelize(add_one_shard_parallel)/jit(main)/add;"}
  ROOT %add.2 = f32[16,64]{1,0} add(f32[16,64]{1,0} %param_1, f32[16,64]{1,0} %broadcast.2), metadata={op_name="parallelize(add_one_shard_parallel)/jit(main)/add;"}
}

ENTRY %main.8_spmd (param: f32[16,64]) -> (f32[16,64]) {
  %param = f32[16,64]{1,0} parameter(0), sharding={devices=[4,1]0,1,2,3}
  %fusion = f32[16,64]{1,0} fusion(f32[16,64]{1,0} %param), kind=kLoop, calls=%fused_computation, metadata={op_name="parallelize(add_one_shard_parallel)/jit(main)/add;"}
  ROOT %tuple = (f32[16,64]{1,0}) tuple(f32[16,64]{1,0} %fusion), frontend_attributes={fingerprint_before_lhs="07b8302033070b680558c6dc80148e81"}, metadata={op_name="tuple.5"}
}

