# sss='#loc = loc(unknown)\n#loc1 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]")\n#loc2 = loc("/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py":3502:0)\n#loc3 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]")\n#loc4 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/loss.py":42:0)\n#loc5 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]")\n#loc6 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]")\n#loc7 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":98:0)\n#loc102 = loc(fused[#loc1, #loc2])\n#loc103 = loc(fused[#loc3, #loc4])\n#loc104 = loc(fused[#loc5, #loc4])\n#loc105 = loc(fused[#loc6, #loc4])\n#loc106 = loc(fused[#loc6, #loc7])\nmodule @train_step_shard_parallel attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {\n  func.func private @region_0.93(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc102)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_1.117(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<f32> loc(#loc103)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_2.128(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc104)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_3.140(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc104)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_4.146(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc105)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_5.155(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc104)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_6.162(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc104)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_7.170(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc106)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func @main(%arg0: tensor<i32> loc(unknown), %arg1: tensor<3x3x3x32xf32> loc(unknown), %arg2: tensor<3x3x32x64xf32> loc(unknown), %arg3: tensor<10xf32> loc(unknown), %arg4: tensor<64x10xf32> loc(unknown), %arg5: tensor<i32> loc(unknown), %arg6: tensor<3x3x3x32xf32> loc(unknown), %arg7: tensor<3x3x32x64xf32> loc(unknown), %arg8: tensor<10xf32> loc(unknown), %arg9: tensor<64x10xf32> loc(unknown), %arg10: tensor<3x3x3x32xf32> loc(unknown), %arg11: tensor<3x3x32x64xf32> loc(unknown), %arg12: tensor<10xf32> loc(unknown), %arg13: tensor<64x10xf32> loc(unknown), %arg14: tensor<100x32x32x3xf32> loc(unknown), %arg15: tensor<100xi32> loc(unknown)) -> tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> attributes {xla_entry_computation_result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>]} {\n    %0 = stablehlo.constant dense<1> : tensor<i32> loc(#loc8)\n    %1 = stablehlo.add %arg0, %0 : tensor<i32> loc(#loc107)\n    %2 = stablehlo.convolution(%arg14, %arg1) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<3x3x3x32xf32>) -> tensor<100x32x32x32xf32> loc(#loc108)\n    %3 = stablehlo.convolution(%2, %arg2) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc108)\n    %4 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc13)\n    %5 = stablehlo.constant dense<0.000000e+00> : tensor<100x32x32x64xf32> loc(#loc109)\n    %6 = stablehlo.compare  GT, %3, %5 : (tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xi1> loc(#loc110)\n    %7 = stablehlo.reshape %arg15 : (tensor<100xi32>) -> tensor<100x1xi32> loc(#loc111)\n    %8 = stablehlo.reshape %7 : (tensor<100x1xi32>) -> tensor<100xi32> loc(#loc112)\n    %9 = stablehlo.broadcast_in_dim %8, dims = [0] : (tensor<100xi32>) -> tensor<100x10xi32> loc(#loc112)\n    %10 = stablehlo.iota dim = 0 : tensor<10xi32> loc(#loc113)\n    %11 = stablehlo.reshape %10 : (tensor<10xi32>) -> tensor<1x10xi32> loc(#loc113)\n    %12 = stablehlo.reshape %11 : (tensor<1x10xi32>) -> tensor<10xi32> loc(#loc112)\n    %13 = stablehlo.broadcast_in_dim %12, dims = [1] : (tensor<10xi32>) -> tensor<100x10xi32> loc(#loc112)\n    %14 = stablehlo.compare  EQ, %9, %13 : (tensor<100x10xi32>, tensor<100x10xi32>) -> tensor<100x10xi1> loc(#loc112)\n    %15 = stablehlo.convert %14 : (tensor<100x10xi1>) -> tensor<100x10xf32> loc(#loc114)\n    %16 = stablehlo.constant dense<-0.00999999977> : tensor<f32> loc(#loc22)\n    %17 = stablehlo.constant dense<-0.00999999977> : tensor<100x10xf32> loc(#loc115)\n    %18 = stablehlo.multiply %15, %17 : tensor<100x10xf32> loc(#loc116)\n    %19 = stablehlo.negate %18 : tensor<100x10xf32> loc(#loc117)\n    %20 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc26)\n    %21 = stablehlo.reduce(%19 init: %20) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc104)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc104)\n    %22 = stablehlo.reshape %21 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc118)\n    %23 = stablehlo.maximum %3, %5 : tensor<100x32x32x64xf32> loc(#loc109)\n    %24 = stablehlo.reduce(%23 init: %20) across dimensions = [1, 2] : (tensor<100x32x32x64xf32>, tensor<f32>) -> tensor<100x64xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc1, #loc2]), %arg17: tensor<f32> loc(fused[#loc1, #loc2]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc102)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc102)\n    %25 = stablehlo.constant dense<1.024000e+03> : tensor<f32> loc(#loc28)\n    %26 = stablehlo.constant dense<1.024000e+03> : tensor<100x64xf32> loc(#loc119)\n    %27 = stablehlo.divide %24, %26 : tensor<100x64xf32> loc(#loc119)\n    %28 = stablehlo.dot %27, %arg4, precision = [DEFAULT, DEFAULT] : (tensor<100x64xf32>, tensor<64x10xf32>) -> tensor<100x10xf32> loc(#loc120)\n    %29 = stablehlo.reshape %arg3 : (tensor<10xf32>) -> tensor<1x10xf32> loc(#loc121)\n    %30 = stablehlo.reshape %29 : (tensor<1x10xf32>) -> tensor<10xf32> loc(#loc122)\n    %31 = stablehlo.broadcast_in_dim %30, dims = [1] : (tensor<10xf32>) -> tensor<100x10xf32> loc(#loc122)\n    %32 = stablehlo.add %28, %31 : tensor<100x10xf32> loc(#loc122)\n    %33 = stablehlo.constant dense<1.000000e-07> : tensor<f32> loc(#loc33)\n    %34 = stablehlo.constant dense<1.000000e-07> : tensor<100x10xf32> loc(#loc123)\n    %35 = stablehlo.add %32, %34 : tensor<100x10xf32> loc(#loc123)\n    %36 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc34)\n    %37 = stablehlo.reduce(%35 init: %36) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc3, #loc4]), %arg17: tensor<f32> loc(fused[#loc3, #loc4]))  {\n      %214 = stablehlo.maximum %arg16, %arg17 : tensor<f32> loc(#loc103)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc103)\n    %38 = stablehlo.reshape %37 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc124)\n    %39 = stablehlo.reshape %38 : (tensor<100x1xf32>) -> tensor<100xf32> loc(#loc125)\n    %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc125)\n    %41 = stablehlo.subtract %35, %40 : tensor<100x10xf32> loc(#loc125)\n    %42 = stablehlo.exponential %41 : tensor<100x10xf32> loc(#loc126)\n    %43 = stablehlo.reduce(%42 init: %20) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc104)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc104)\n    %44 = stablehlo.reshape %43 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc124)\n    %45 = stablehlo.divide %22, %44 : tensor<100x1xf32> loc(#loc127)\n    %46 = stablehlo.reduce(%45 init: %20) across dimensions = [1] : (tensor<100x1xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc104)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc104)\n    %47 = stablehlo.broadcast_in_dim %46, dims = [0] : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc115)\n    %48 = stablehlo.multiply %47, %42 : tensor<100x10xf32> loc(#loc116)\n    %49 = stablehlo.add %18, %48 : tensor<100x10xf32> loc(#loc128)\n    %50 = stablehlo.dot_general %49, %arg4, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<100x10xf32>, tensor<64x10xf32>) -> tensor<100x64xf32> loc(#loc129)\n    %51 = stablehlo.divide %50, %26 : tensor<100x64xf32> loc(#loc119)\n    %52 = stablehlo.broadcast_in_dim %51, dims = [0, 3] : (tensor<100x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc130)\n    %53 = stablehlo.select %6, %52, %5 : tensor<100x32x32x64xi1>, tensor<100x32x32x64xf32> loc(#loc131)\n    %54 = stablehlo.reverse %arg2, dims = [0, 1] : tensor<3x3x32x64xf32> loc(#loc132)\n    %55 = stablehlo.convolution(%53, %54) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x64xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x32xf32> loc(#loc133)\n    %56 = stablehlo.convolution(%arg14, %55) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<100x32x32x32xf32>) -> tensor<3x3x3x32xf32> loc(#loc134)\n    %57 = stablehlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc44)\n    %58 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x3x32xf32> loc(#loc135)\n    %59 = stablehlo.multiply %56, %58 : tensor<3x3x3x32xf32> loc(#loc135)\n    %60 = stablehlo.constant dense<0.899999976> : tensor<f32> loc(#loc46)\n    %61 = stablehlo.constant dense<0.899999976> : tensor<3x3x3x32xf32> loc(#loc135)\n    %62 = stablehlo.multiply %arg6, %61 : tensor<3x3x3x32xf32> loc(#loc135)\n    %63 = stablehlo.add %59, %62 : tensor<3x3x3x32xf32> loc(#loc136)\n    %64 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc47)\n    %65 = stablehlo.constant dense<0.899999976> : tensor<f32> loc(#loc48)\n    %66 = stablehlo.constant dense<2147483647> : tensor<i32> loc(#loc49)\n    %67 = stablehlo.compare  LT, %arg5, %66 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc137)\n    %68 = stablehlo.add %arg5, %0 : tensor<i32> loc(#loc138)\n    %69 = stablehlo.select %67, %68, %66 : tensor<i1>, tensor<i32> loc(#loc139)\n    %70 = stablehlo.convert %69 : (tensor<i32>) -> tensor<f32> loc(#loc140)\n    %71 = stablehlo.power %65, %70 : tensor<f32> loc(#loc141)\n    %72 = stablehlo.subtract %64, %71 : tensor<f32> loc(#loc142)\n    %73 = stablehlo.broadcast_in_dim %72, dims = [] : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc143)\n    %74 = stablehlo.divide %63, %73 : tensor<3x3x3x32xf32> loc(#loc143)\n    %75 = stablehlo.multiply %56, %56 : tensor<3x3x3x32xf32> loc(#loc144)\n    %76 = stablehlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc56)\n    %77 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x3x32xf32> loc(#loc145)\n    %78 = stablehlo.multiply %75, %77 : tensor<3x3x3x32xf32> loc(#loc145)\n    %79 = stablehlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc58)\n    %80 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x3x32xf32> loc(#loc145)\n    %81 = stablehlo.multiply %arg10, %80 : tensor<3x3x3x32xf32> loc(#loc145)\n    %82 = stablehlo.add %78, %81 : tensor<3x3x3x32xf32> loc(#loc146)\n    %83 = stablehlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc59)\n    %84 = stablehlo.convert %69 : (tensor<i32>) -> tensor<f32> loc(#loc140)\n    %85 = stablehlo.power %83, %84 : tensor<f32> loc(#loc141)\n    %86 = stablehlo.subtract %64, %85 : tensor<f32> loc(#loc142)\n    %87 = stablehlo.broadcast_in_dim %86, dims = [] : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc143)\n    %88 = stablehlo.divide %82, %87 : tensor<3x3x3x32xf32> loc(#loc143)\n    %89 = stablehlo.sqrt %88 : tensor<3x3x3x32xf32> loc(#loc147)\n    %90 = stablehlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc62)\n    %91 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x3x32xf32> loc(#loc148)\n    %92 = stablehlo.add %89, %91 : tensor<3x3x3x32xf32> loc(#loc148)\n    %93 = stablehlo.divide %74, %92 : tensor<3x3x3x32xf32> loc(#loc149)\n    %94 = stablehlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc63)\n    %95 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x3x32xf32> loc(#loc150)\n    %96 = stablehlo.multiply %arg1, %95 : tensor<3x3x3x32xf32> loc(#loc150)\n    %97 = stablehlo.add %93, %96 : tensor<3x3x3x32xf32> loc(#loc151)\n    %98 = stablehlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc65)\n    %99 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x3x32xf32> loc(#loc152)\n    %100 = stablehlo.multiply %97, %99 : tensor<3x3x3x32xf32> loc(#loc152)\n    %101 = stablehlo.add %arg1, %100 : tensor<3x3x3x32xf32> loc(#loc153)\n    %102 = stablehlo.convolution(%2, %53) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<100x32x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc134)\n    %103 = stablehlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc68)\n    %104 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x32x64xf32> loc(#loc135)\n    %105 = stablehlo.multiply %102, %104 : tensor<3x3x32x64xf32> loc(#loc135)\n    %106 = stablehlo.constant dense<0.899999976> : tensor<f32> loc(#loc69)\n    %107 = stablehlo.constant dense<0.899999976> : tensor<3x3x32x64xf32> loc(#loc135)\n    %108 = stablehlo.multiply %arg7, %107 : tensor<3x3x32x64xf32> loc(#loc135)\n    %109 = stablehlo.add %105, %108 : tensor<3x3x32x64xf32> loc(#loc136)\n    %110 = stablehlo.broadcast_in_dim %72, dims = [] : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc143)\n    %111 = stablehlo.divide %109, %110 : tensor<3x3x32x64xf32> loc(#loc143)\n    %112 = stablehlo.multiply %102, %102 : tensor<3x3x32x64xf32> loc(#loc144)\n    %113 = stablehlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc70)\n    %114 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x32x64xf32> loc(#loc145)\n    %115 = stablehlo.multiply %112, %114 : tensor<3x3x32x64xf32> loc(#loc145)\n    %116 = stablehlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc71)\n    %117 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x32x64xf32> loc(#loc145)\n    %118 = stablehlo.multiply %arg11, %117 : tensor<3x3x32x64xf32> loc(#loc145)\n    %119 = stablehlo.add %115, %118 : tensor<3x3x32x64xf32> loc(#loc146)\n    %120 = stablehlo.broadcast_in_dim %86, dims = [] : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc143)\n    %121 = stablehlo.divide %119, %120 : tensor<3x3x32x64xf32> loc(#loc143)\n    %122 = stablehlo.sqrt %121 : tensor<3x3x32x64xf32> loc(#loc147)\n    %123 = stablehlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc72)\n    %124 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x32x64xf32> loc(#loc148)\n    %125 = stablehlo.add %122, %124 : tensor<3x3x32x64xf32> loc(#loc148)\n    %126 = stablehlo.divide %111, %125 : tensor<3x3x32x64xf32> loc(#loc149)\n    %127 = stablehlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc73)\n    %128 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x32x64xf32> loc(#loc150)\n    %129 = stablehlo.multiply %arg2, %128 : tensor<3x3x32x64xf32> loc(#loc150)\n    %130 = stablehlo.add %126, %129 : tensor<3x3x32x64xf32> loc(#loc151)\n    %131 = stablehlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc74)\n    %132 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x32x64xf32> loc(#loc152)\n    %133 = stablehlo.multiply %130, %132 : tensor<3x3x32x64xf32> loc(#loc152)\n    %134 = stablehlo.add %arg2, %133 : tensor<3x3x32x64xf32> loc(#loc153)\n    %135 = stablehlo.reduce(%49 init: %20) across dimensions = [0] : (tensor<100x10xf32>, tensor<f32>) -> tensor<10xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc7]), %arg17: tensor<f32> loc(fused[#loc6, #loc7]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc106)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc106)\n    %136 = stablehlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc75)\n    %137 = stablehlo.constant dense<1.000000e-01> : tensor<10xf32> loc(#loc76)\n    %138 = stablehlo.multiply %135, %137 : tensor<10xf32> loc(#loc135)\n    %139 = stablehlo.constant dense<0.899999976> : tensor<f32> loc(#loc77)\n    %140 = stablehlo.constant dense<0.899999976> : tensor<10xf32> loc(#loc78)\n    %141 = stablehlo.multiply %arg8, %140 : tensor<10xf32> loc(#loc135)\n    %142 = stablehlo.add %138, %141 : tensor<10xf32> loc(#loc136)\n    %143 = stablehlo.broadcast_in_dim %72, dims = [] : (tensor<f32>) -> tensor<10xf32> loc(#loc143)\n    %144 = stablehlo.divide %142, %143 : tensor<10xf32> loc(#loc143)\n    %145 = stablehlo.multiply %135, %135 : tensor<10xf32> loc(#loc144)\n    %146 = stablehlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc79)\n    %147 = stablehlo.constant dense<1.000000e-03> : tensor<10xf32> loc(#loc80)\n    %148 = stablehlo.multiply %145, %147 : tensor<10xf32> loc(#loc145)\n    %149 = stablehlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc81)\n    %150 = stablehlo.constant dense<9.990000e-01> : tensor<10xf32> loc(#loc82)\n    %151 = stablehlo.multiply %arg12, %150 : tensor<10xf32> loc(#loc145)\n    %152 = stablehlo.add %148, %151 : tensor<10xf32> loc(#loc146)\n    %153 = stablehlo.broadcast_in_dim %86, dims = [] : (tensor<f32>) -> tensor<10xf32> loc(#loc143)\n    %154 = stablehlo.divide %152, %153 : tensor<10xf32> loc(#loc143)\n    %155 = stablehlo.sqrt %154 : tensor<10xf32> loc(#loc147)\n    %156 = stablehlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc83)\n    %157 = stablehlo.constant dense<9.99999993E-9> : tensor<10xf32> loc(#loc84)\n    %158 = stablehlo.add %155, %157 : tensor<10xf32> loc(#loc148)\n    %159 = stablehlo.divide %144, %158 : tensor<10xf32> loc(#loc149)\n    %160 = stablehlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc85)\n    %161 = stablehlo.constant dense<5.000000e-04> : tensor<10xf32> loc(#loc86)\n    %162 = stablehlo.multiply %arg3, %161 : tensor<10xf32> loc(#loc150)\n    %163 = stablehlo.add %159, %162 : tensor<10xf32> loc(#loc151)\n    %164 = stablehlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc87)\n    %165 = stablehlo.constant dense<-3.000000e-04> : tensor<10xf32> loc(#loc88)\n    %166 = stablehlo.multiply %163, %165 : tensor<10xf32> loc(#loc152)\n    %167 = stablehlo.add %arg3, %166 : tensor<10xf32> loc(#loc153)\n    %168 = stablehlo.dot_general %49, %27, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<100x10xf32>, tensor<100x64xf32>) -> tensor<10x64xf32> loc(#loc154)\n    %169 = stablehlo.transpose %168, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : (tensor<10x64xf32>) -> tensor<64x10xf32> loc(#loc155)\n    %170 = stablehlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc91)\n    %171 = stablehlo.constant dense<1.000000e-01> : tensor<64x10xf32> loc(#loc135)\n    %172 = stablehlo.multiply %169, %171 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc135)\n    %173 = stablehlo.constant dense<0.899999976> : tensor<f32> loc(#loc92)\n    %174 = stablehlo.constant dense<0.899999976> : tensor<64x10xf32> loc(#loc135)\n    %175 = stablehlo.multiply %arg9, %174 : tensor<64x10xf32> loc(#loc135)\n    %176 = stablehlo.add %172, %175 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc136)\n    %177 = stablehlo.broadcast_in_dim %72, dims = [] : (tensor<f32>) -> tensor<64x10xf32> loc(#loc143)\n    %178 = stablehlo.divide %176, %177 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc143)\n    %179 = stablehlo.multiply %169, %169 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc144)\n    %180 = stablehlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc93)\n    %181 = stablehlo.constant dense<1.000000e-03> : tensor<64x10xf32> loc(#loc145)\n    %182 = stablehlo.multiply %179, %181 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc145)\n    %183 = stablehlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc94)\n    %184 = stablehlo.constant dense<9.990000e-01> : tensor<64x10xf32> loc(#loc145)\n    %185 = stablehlo.multiply %arg13, %184 : tensor<64x10xf32> loc(#loc145)\n    %186 = stablehlo.add %182, %185 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc146)\n    %187 = stablehlo.broadcast_in_dim %86, dims = [] : (tensor<f32>) -> tensor<64x10xf32> loc(#loc143)\n    %188 = stablehlo.divide %186, %187 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc143)\n    %189 = stablehlo.sqrt %188 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc147)\n    %190 = stablehlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc95)\n    %191 = stablehlo.constant dense<9.99999993E-9> : tensor<64x10xf32> loc(#loc148)\n    %192 = stablehlo.add %189, %191 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc148)\n    %193 = stablehlo.divide %178, %192 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc149)\n    %194 = stablehlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc96)\n    %195 = stablehlo.constant dense<5.000000e-04> : tensor<64x10xf32> loc(#loc150)\n    %196 = stablehlo.multiply %arg4, %195 : tensor<64x10xf32> loc(#loc150)\n    %197 = stablehlo.add %193, %196 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc151)\n    %198 = stablehlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc97)\n    %199 = stablehlo.constant dense<-3.000000e-04> : tensor<64x10xf32> loc(#loc152)\n    %200 = stablehlo.multiply %197, %199 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc152)\n    %201 = stablehlo.add %arg4, %200 : tensor<64x10xf32> loc(#loc153)\n    %202 = stablehlo.log %44 : tensor<100x1xf32> loc(#loc156)\n    %203 = stablehlo.reshape %202 : (tensor<100x1xf32>) -> tensor<100xf32> loc(#loc125)\n    %204 = stablehlo.broadcast_in_dim %203, dims = [0] : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc125)\n    %205 = stablehlo.subtract %41, %204 : tensor<100x10xf32> loc(#loc125)\n    %206 = stablehlo.multiply %15, %205 : tensor<100x10xf32> loc(#loc116)\n    %207 = stablehlo.reduce(%206 init: %20) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc104)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc104)\n    %208 = stablehlo.negate %207 : tensor<100xf32> loc(#loc117)\n    %209 = stablehlo.reduce(%208 init: %20) across dimensions = [0] : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc4]), %arg17: tensor<f32> loc(fused[#loc6, #loc4]))  {\n      %214 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc105)\n      stablehlo.return %214 : tensor<f32> loc(#loc)\n    } loc(#loc105)\n    %210 = stablehlo.constant dense<1.000000e+02> : tensor<f32> loc(#loc99)\n    %211 = stablehlo.divide %209, %210 : tensor<f32> loc(#loc127)\n    %212 = stablehlo.divide %211, %64 : tensor<f32> loc(#loc157)\n    %213 = stablehlo.tuple %1, %101, %134, %167, %201, %69, %63, %109, %142, %176, %82, %119, %152, %186, %212 {result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>], xla_shape = "(s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{1,0}, /*index=5*/s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, /*index=10*/f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, f32[])"} : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc101)\n    return %213 : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc)\n  } loc(#loc)\n} loc(#loc)\n#loc8 = loc("constant.82")\n#loc9 = loc("parallelize(train_step_shard_parallel)/jit(main)/add")\n#loc10 = loc("/usr/local/lib/python3.9/dist-packages/flax/training/train_state.py":77:0)\n#loc11 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc12 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/conv.py":212:0)\n#loc13 = loc("constant.73")\n#loc14 = loc("parallelize(train_step_shard_parallel)/jit(main)/max")\n#loc15 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/activate.py":25:0)\n#loc16 = loc("parallelize(train_step_shard_parallel)/jit(main)/gt")\n#loc17 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 1) broadcast_dimensions=(0,)]")\n#loc18 = loc("/data/hejing/distri/alpa/vgg16-alpa.py":123:0)\n#loc19 = loc("parallelize(train_step_shard_parallel)/jit(main)/eq")\n#loc20 = loc("parallelize(train_step_shard_parallel)/jit(main)/iota[dtype=int32 shape=(1, 10) dimension=1]")\n#loc21 = loc("parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]")\n#loc22 = loc("constant.77")\n#loc23 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 10) broadcast_dimensions=(0,)]")\n#loc24 = loc("parallelize(train_step_shard_parallel)/jit(main)/mul")\n#loc25 = loc("parallelize(train_step_shard_parallel)/jit(main)/neg")\n#loc26 = loc("constant.86")\n#loc27 = loc("parallelize(train_step_shard_parallel)/jit(main)/reshape[new_sizes=(100, 1) dimensions=None]")\n#loc28 = loc("constant.75")\n#loc29 = loc("parallelize(train_step_shard_parallel)/jit(main)/div")\n#loc30 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc31 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":95:0)\n#loc32 = loc("parallelize(train_step_shard_parallel)/jit(main)/reshape[new_sizes=(1, 10) dimensions=None]")\n#loc33 = loc("constant.80")\n#loc34 = loc("constant.85")\n#loc35 = loc("parallelize(train_step_shard_parallel)/jit(main)/sub")\n#loc36 = loc("parallelize(train_step_shard_parallel)/jit(main)/exp")\n#loc37 = loc("parallelize(train_step_shard_parallel)/jit(main)/add_any")\n#loc38 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]")\n#loc39 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 32, 32, 64) broadcast_dimensions=(0, 3)]")\n#loc40 = loc("parallelize(train_step_shard_parallel)/jit(main)/select_n")\n#loc41 = loc("parallelize(train_step_shard_parallel)/jit(main)/rev[dimensions=(0, 1)]")\n#loc42 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(2, 3, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc43 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc44 = loc("constant.71")\n#loc45 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":84:0)\n#loc46 = loc("constant.69")\n#loc47 = loc("constant.79")\n#loc48 = loc("constant.87")\n#loc49 = loc("constant.83")\n#loc50 = loc("parallelize(train_step_shard_parallel)/jit(main)/lt")\n#loc51 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/numerics.py":118:0)\n#loc52 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":118:0)\n#loc53 = loc("parallelize(train_step_shard_parallel)/jit(main)/pow")\n#loc54 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":122:0)\n#loc55 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":98:0)\n#loc56 = loc("constant.55")\n#loc57 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":107:0)\n#loc58 = loc("constant.53")\n#loc59 = loc("constant.88")\n#loc60 = loc("parallelize(train_step_shard_parallel)/jit(main)/sqrt")\n#loc61 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":348:0)\n#loc62 = loc("constant.39")\n#loc63 = loc("constant.31")\n#loc64 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":770:0)\n#loc65 = loc("constant.23")\n#loc66 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":518:0)\n#loc67 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/update.py":43:0)\n#loc68 = loc("constant.67")\n#loc69 = loc("constant.65")\n#loc70 = loc("constant.51")\n#loc71 = loc("constant.49")\n#loc72 = loc("constant.37")\n#loc73 = loc("constant.29")\n#loc74 = loc("constant.21")\n#loc75 = loc("constant.63")\n#loc76 = loc("broadcast.64")\n#loc77 = loc("constant.61")\n#loc78 = loc("broadcast.62")\n#loc79 = loc("constant.47")\n#loc80 = loc("broadcast.48")\n#loc81 = loc("constant.45")\n#loc82 = loc("broadcast.46")\n#loc83 = loc("constant.35")\n#loc84 = loc("broadcast.36")\n#loc85 = loc("constant.27")\n#loc86 = loc("broadcast.28")\n#loc87 = loc("constant.19")\n#loc88 = loc("broadcast.20")\n#loc89 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc90 = loc("parallelize(train_step_shard_parallel)/jit(main)/transpose[permutation=(1, 0)]")\n#loc91 = loc("constant.59")\n#loc92 = loc("constant.57")\n#loc93 = loc("constant.43")\n#loc94 = loc("constant.41")\n#loc95 = loc("constant.33")\n#loc96 = loc("constant.25")\n#loc97 = loc("constant.17")\n#loc98 = loc("parallelize(train_step_shard_parallel)/jit(main)/log")\n#loc99 = loc("constant.84")\n#loc100 = loc("/data/hejing/distri/alpa/vgg16-alpa.py":124:0)\n#loc101 = loc("tuple.267")\n#loc107 = loc(fused[#loc9, #loc10])\n#loc108 = loc(fused[#loc11, #loc12])\n#loc109 = loc(fused[#loc14, #loc15])\n#loc110 = loc(fused[#loc16, #loc15])\n#loc111 = loc(fused[#loc17, #loc18])\n#loc112 = loc(fused[#loc19, #loc18])\n#loc113 = loc(fused[#loc20, #loc18])\n#loc114 = loc(fused[#loc21, #loc18])\n#loc115 = loc(fused[#loc23, #loc4])\n#loc116 = loc(fused[#loc24, #loc4])\n#loc117 = loc(fused[#loc25, #loc4])\n#loc118 = loc(fused[#loc27, #loc4])\n#loc119 = loc(fused[#loc29, #loc2])\n#loc120 = loc(fused[#loc30, #loc31])\n#loc121 = loc(fused[#loc32, #loc7])\n#loc122 = loc(fused[#loc9, #loc7])\n#loc123 = loc(fused[#loc9, #loc4])\n#loc124 = loc(fused[#loc17, #loc4])\n#loc125 = loc(fused[#loc35, #loc4])\n#loc126 = loc(fused[#loc36, #loc4])\n#loc127 = loc(fused[#loc29, #loc4])\n#loc128 = loc(fused[#loc37, #loc4])\n#loc129 = loc(fused[#loc38, #loc31])\n#loc130 = loc(fused[#loc39, #loc2])\n#loc131 = loc(fused[#loc40, #loc15])\n#loc132 = loc(fused[#loc41, #loc12])\n#loc133 = loc(fused[#loc42, #loc12])\n#loc134 = loc(fused[#loc43, #loc12])\n#loc135 = loc(fused[#loc24, #loc45])\n#loc136 = loc(fused[#loc9, #loc45])\n#loc137 = loc(fused[#loc50, #loc51])\n#loc138 = loc(fused[#loc9, #loc51])\n#loc139 = loc(fused[#loc40, #loc51])\n#loc140 = loc(fused[#loc21, #loc52])\n#loc141 = loc(fused[#loc53, #loc52])\n#loc142 = loc(fused[#loc35, #loc52])\n#loc143 = loc(fused[#loc29, #loc54])\n#loc144 = loc(fused[#loc24, #loc55])\n#loc145 = loc(fused[#loc24, #loc57])\n#loc146 = loc(fused[#loc9, #loc57])\n#loc147 = loc(fused[#loc60, #loc61])\n#loc148 = loc(fused[#loc9, #loc61])\n#loc149 = loc(fused[#loc29, #loc61])\n#loc150 = loc(fused[#loc24, #loc64])\n#loc151 = loc(fused[#loc9, #loc64])\n#loc152 = loc(fused[#loc24, #loc66])\n#loc153 = loc(fused[#loc9, #loc67])\n#loc154 = loc(fused[#loc89, #loc31])\n#loc155 = loc(fused[#loc90, #loc31])\n#loc156 = loc(fused[#loc98, #loc4])\n#loc157 = loc(fused[#loc29, #loc100])\n'
# print(sss)
# print('-----------------------------------------------------------')
# sss2='#loc = loc(unknown)\n#loc1 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]")\n#loc2 = loc("/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py":3502:0)\n#loc3 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]")\n#loc4 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/loss.py":42:0)\n#loc5 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]")\n#loc6 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]")\n#loc7 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":98:0)\n#loc110 = loc(fused[#loc1, #loc2])\n#loc111 = loc(fused[#loc3, #loc4])\n#loc112 = loc(fused[#loc5, #loc4])\n#loc113 = loc(fused[#loc6, #loc4])\n#loc114 = loc(fused[#loc6, #loc7])\nmodule @train_step_shard_parallel attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {\n  func.func private @region_0.93(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc110)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_1.117(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<f32> loc(#loc111)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_2.128(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_3.140(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_4.146(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc113)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_5.155(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_7.170(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc114)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func @main(%arg0: tensor<i32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg1: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg2: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg3: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg4: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg5: tensor<i32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg6: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg7: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg8: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg9: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg10: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg11: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg12: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg13: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg14: tensor<100x32x32x3xf32> {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} loc(unknown), %arg15: tensor<100xi32> {mhlo.sharding = "{devices=[2]0,1}"} loc(unknown)) -> (tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> {mhlo.sharding = "{{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=5*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=10*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}}"}) attributes {xla_entry_computation_result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>]} {\n    %0 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1> : tensor<i32> loc(#loc8)\n    %1 = stablehlo.add %arg0, %0 {mhlo.sharding = "{replicated}"} : tensor<i32> loc(#loc115)\n    %2 = stablehlo.convolution(%arg14, %arg1) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<3x3x3x32xf32>) -> tensor<100x32x32x32xf32> loc(#loc116)\n    %3 = stablehlo.convolution(%2, %arg2) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc116)\n    %4 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc13)\n    %5 = stablehlo.constant dense<0.000000e+00> : tensor<100x32x32x64xf32> loc(#loc117)\n    %6 = stablehlo.compare  GT, %3, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : (tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xi1> loc(#loc118)\n    %7 = stablehlo.broadcast_in_dim %arg15, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xi32>) -> tensor<100x10xi32> loc(#loc119)\n    %8 = stablehlo.iota dim = 1 : tensor<100x10xi32> loc(#loc119)\n    %9 = stablehlo.compare  EQ, %7, %8 {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x10xi32>, tensor<100x10xi32>) -> tensor<100x10xi1> loc(#loc119)\n    %10 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-0.00999999977> : tensor<f32> loc(#loc19)\n    %11 = stablehlo.constant dense<-0.00999999977> : tensor<100x10xf32> loc(#loc120)\n    %12 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc21)\n    %13 = stablehlo.constant dense<0.000000e+00> : tensor<100x10xf32> loc(#loc22)\n    %14 = stablehlo.select %9, %11, %13 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xi1>, tensor<100x10xf32> loc(#loc121)\n    %15 = stablehlo.negate %14 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc122)\n    %16 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc25)\n    %17 = stablehlo.reduce(%15 init: %16) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %18 = stablehlo.maximum %3, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : tensor<100x32x32x64xf32> loc(#loc117)\n    %19 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc26)\n    %20 = stablehlo.reduce(%18 init: %19) across dimensions = [1, 2] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x32x32x64xf32>, tensor<f32>) -> tensor<100x64xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc1, #loc2]), %arg17: tensor<f32> loc(fused[#loc1, #loc2]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc110)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc110)\n    %21 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.765625E-4> : tensor<f32> loc(#loc27)\n    %22 = stablehlo.constant dense<9.765625E-4> : tensor<100x64xf32> loc(#loc123)\n    %23 = stablehlo.multiply %20, %22 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x64xf32> loc(#loc123)\n    %24 = stablehlo.dot %23, %arg4, precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x64xf32>, tensor<64x10xf32>) -> tensor<100x10xf32> loc(#loc124)\n    %25 = stablehlo.broadcast_in_dim %arg3, dims = [1] {mhlo.sharding = "{replicated}"} : (tensor<10xf32>) -> tensor<100x10xf32> loc(#loc125)\n    %26 = stablehlo.add %24, %25 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc125)\n    %27 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-07> : tensor<f32> loc(#loc31)\n    %28 = stablehlo.constant dense<1.000000e-07> : tensor<100x10xf32> loc(#loc126)\n    %29 = stablehlo.add %26, %28 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc126)\n    %30 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0xFF800000> : tensor<f32> loc(#loc32)\n    %31 = stablehlo.reduce(%29 init: %30) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc3, #loc4]), %arg17: tensor<f32> loc(fused[#loc3, #loc4]))  {\n      %207 = stablehlo.maximum %arg16, %arg17 : tensor<f32> loc(#loc111)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc111)\n    %32 = stablehlo.broadcast_in_dim %31, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc127)\n    %33 = stablehlo.subtract %29, %32 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc127)\n    %34 = stablehlo.exponential %33 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc128)\n    %35 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc35)\n    %36 = stablehlo.reduce(%34 init: %35) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %37 = stablehlo.divide %17, %36 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc129)\n    %38 = stablehlo.broadcast_in_dim %37, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc120)\n    %39 = stablehlo.multiply %38, %34 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc121)\n    %40 = stablehlo.add %14, %39 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc130)\n    %41 = stablehlo.dot_general %40, %arg4, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x10xf32>, tensor<64x10xf32>) -> tensor<100x64xf32> loc(#loc131)\n    %42 = stablehlo.multiply %41, %22 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x64xf32> loc(#loc123)\n    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 3] {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : (tensor<100x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc132)\n    %44 = stablehlo.select %6, %43, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : tensor<100x32x32x64xi1>, tensor<100x32x32x64xf32> loc(#loc133)\n    %45 = stablehlo.reverse %arg2, dims = [0, 1] : tensor<3x3x32x64xf32> loc(#loc134)\n    %46 = stablehlo.convolution(%44, %45) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x64xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x32xf32> loc(#loc135)\n    %47 = stablehlo.convolution(%arg14, %46) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{replicated}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<100x32x32x32xf32>) -> tensor<3x3x3x32xf32> loc(#loc136)\n    %48 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc43)\n    %49 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x3x32xf32> loc(#loc137)\n    %50 = stablehlo.multiply %47, %49 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc137)\n    %51 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc45)\n    %52 = stablehlo.constant dense<0.899999976> : tensor<3x3x3x32xf32> loc(#loc137)\n    %53 = stablehlo.multiply %arg6, %52 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc137)\n    %54 = stablehlo.add %50, %53 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc138)\n    %55 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e+00> : tensor<f32> loc(#loc46)\n    %56 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc47)\n    %57 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<2147483647> : tensor<i32> loc(#loc48)\n    %58 = stablehlo.compare  LT, %arg5, %57 {mhlo.sharding = "{replicated}"} : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc139)\n    %59 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1> : tensor<i32> loc(#loc51)\n    %60 = stablehlo.add %arg5, %59 {mhlo.sharding = "{replicated}"} : tensor<i32> loc(#loc140)\n    %61 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<2147483647> : tensor<i32> loc(#loc52)\n    %62 = stablehlo.select %58, %60, %61 {mhlo.sharding = "{replicated}"} : tensor<i1>, tensor<i32> loc(#loc141)\n    %63 = stablehlo.convert %62 {mhlo.sharding = "{replicated}"} : (tensor<i32>) -> tensor<f32> loc(#loc142)\n    %64 = stablehlo.power %56, %63 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc143)\n    %65 = stablehlo.subtract %55, %64 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc144)\n    %66 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc145)\n    %67 = stablehlo.multiply %47, %47 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc146)\n    %68 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc58)\n    %69 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x3x32xf32> loc(#loc147)\n    %70 = stablehlo.multiply %67, %69 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc147)\n    %71 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc60)\n    %72 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x3x32xf32> loc(#loc147)\n    %73 = stablehlo.multiply %arg10, %72 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc147)\n    %74 = stablehlo.add %70, %73 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc148)\n    %75 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e+00> : tensor<f32> loc(#loc61)\n    %76 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc62)\n    %77 = stablehlo.power %76, %63 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc143)\n    %78 = stablehlo.subtract %75, %77 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc144)\n    %79 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc145)\n    %80 = stablehlo.divide %74, %79 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc145)\n    %81 = stablehlo.sqrt %80 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc149)\n    %82 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc65)\n    %83 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x3x32xf32> loc(#loc150)\n    %84 = stablehlo.add %81, %83 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc150)\n    %85 = stablehlo.multiply %66, %84 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc66)\n    %86 = stablehlo.divide %54, %85 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc151)\n    %87 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc67)\n    %88 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x3x32xf32> loc(#loc152)\n    %89 = stablehlo.multiply %arg1, %88 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc152)\n    %90 = stablehlo.add %86, %89 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc153)\n    %91 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc69)\n    %92 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x3x32xf32> loc(#loc154)\n    %93 = stablehlo.multiply %90, %92 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc154)\n    %94 = stablehlo.add %arg1, %93 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc155)\n    %95 = stablehlo.convolution(%2, %44) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{replicated}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<100x32x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc136)\n    %96 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc72)\n    %97 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x32x64xf32> loc(#loc137)\n    %98 = stablehlo.multiply %95, %97 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc137)\n    %99 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc73)\n    %100 = stablehlo.constant dense<0.899999976> : tensor<3x3x32x64xf32> loc(#loc137)\n    %101 = stablehlo.multiply %arg7, %100 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc137)\n    %102 = stablehlo.add %98, %101 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc138)\n    %103 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc145)\n    %104 = stablehlo.multiply %95, %95 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc146)\n    %105 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc74)\n    %106 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x32x64xf32> loc(#loc147)\n    %107 = stablehlo.multiply %104, %106 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc147)\n    %108 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc75)\n    %109 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x32x64xf32> loc(#loc147)\n    %110 = stablehlo.multiply %arg11, %109 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc147)\n    %111 = stablehlo.add %107, %110 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc148)\n    %112 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc145)\n    %113 = stablehlo.divide %111, %112 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc145)\n    %114 = stablehlo.sqrt %113 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc149)\n    %115 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc76)\n    %116 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x32x64xf32> loc(#loc150)\n    %117 = stablehlo.add %114, %116 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc150)\n    %118 = stablehlo.multiply %103, %117 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc77)\n    %119 = stablehlo.divide %102, %118 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc151)\n    %120 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc78)\n    %121 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x32x64xf32> loc(#loc152)\n    %122 = stablehlo.multiply %arg2, %121 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc152)\n    %123 = stablehlo.add %119, %122 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc153)\n    %124 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc79)\n    %125 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x32x64xf32> loc(#loc154)\n    %126 = stablehlo.multiply %123, %125 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc154)\n    %127 = stablehlo.add %arg2, %126 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc155)\n    %128 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc80)\n    %129 = stablehlo.reduce(%40 init: %128) across dimensions = [0] {mhlo.sharding = "{replicated}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<10xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc7]), %arg17: tensor<f32> loc(fused[#loc6, #loc7]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc114)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc114)\n    %130 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc81)\n    %131 = stablehlo.constant dense<1.000000e-01> : tensor<10xf32> loc(#loc82)\n    %132 = stablehlo.multiply %129, %131 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc137)\n    %133 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc83)\n    %134 = stablehlo.constant dense<0.899999976> : tensor<10xf32> loc(#loc84)\n    %135 = stablehlo.multiply %arg8, %134 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc137)\n    %136 = stablehlo.add %132, %135 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc138)\n    %137 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<10xf32> loc(#loc145)\n    %138 = stablehlo.multiply %129, %129 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc146)\n    %139 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc85)\n    %140 = stablehlo.constant dense<1.000000e-03> : tensor<10xf32> loc(#loc86)\n    %141 = stablehlo.multiply %138, %140 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc147)\n    %142 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc87)\n    %143 = stablehlo.constant dense<9.990000e-01> : tensor<10xf32> loc(#loc88)\n    %144 = stablehlo.multiply %arg12, %143 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc147)\n    %145 = stablehlo.add %141, %144 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc148)\n    %146 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<10xf32> loc(#loc145)\n    %147 = stablehlo.divide %145, %146 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc145)\n    %148 = stablehlo.sqrt %147 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc149)\n    %149 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc89)\n    %150 = stablehlo.constant dense<9.99999993E-9> : tensor<10xf32> loc(#loc90)\n    %151 = stablehlo.add %148, %150 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc150)\n    %152 = stablehlo.multiply %137, %151 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc91)\n    %153 = stablehlo.divide %136, %152 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc151)\n    %154 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc92)\n    %155 = stablehlo.constant dense<5.000000e-04> : tensor<10xf32> loc(#loc93)\n    %156 = stablehlo.multiply %arg3, %155 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc152)\n    %157 = stablehlo.add %153, %156 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc153)\n    %158 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc94)\n    %159 = stablehlo.constant dense<-3.000000e-04> : tensor<10xf32> loc(#loc95)\n    %160 = stablehlo.multiply %157, %159 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc154)\n    %161 = stablehlo.add %arg3, %160 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc155)\n    %162 = stablehlo.dot_general %23, %40, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : (tensor<100x64xf32>, tensor<100x10xf32>) -> tensor<64x10xf32> loc(#loc156)\n    %163 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc97)\n    %164 = stablehlo.constant dense<1.000000e-01> : tensor<64x10xf32> loc(#loc137)\n    %165 = stablehlo.multiply %162, %164 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc137)\n    %166 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc98)\n    %167 = stablehlo.constant dense<0.899999976> : tensor<64x10xf32> loc(#loc137)\n    %168 = stablehlo.multiply %arg9, %167 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc137)\n    %169 = stablehlo.add %165, %168 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc138)\n    %170 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc145)\n    %171 = stablehlo.multiply %162, %162 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc146)\n    %172 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc99)\n    %173 = stablehlo.constant dense<1.000000e-03> : tensor<64x10xf32> loc(#loc147)\n    %174 = stablehlo.multiply %171, %173 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc147)\n    %175 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc100)\n    %176 = stablehlo.constant dense<9.990000e-01> : tensor<64x10xf32> loc(#loc147)\n    %177 = stablehlo.multiply %arg13, %176 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc147)\n    %178 = stablehlo.add %174, %177 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc148)\n    %179 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc145)\n    %180 = stablehlo.divide %178, %179 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc145)\n    %181 = stablehlo.sqrt %180 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc149)\n    %182 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc101)\n    %183 = stablehlo.constant dense<9.99999993E-9> : tensor<64x10xf32> loc(#loc150)\n    %184 = stablehlo.add %181, %183 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc150)\n    %185 = stablehlo.multiply %170, %184 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc102)\n    %186 = stablehlo.divide %169, %185 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc151)\n    %187 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc103)\n    %188 = stablehlo.constant dense<5.000000e-04> : tensor<64x10xf32> loc(#loc152)\n    %189 = stablehlo.multiply %arg4, %188 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc152)\n    %190 = stablehlo.add %186, %189 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc153)\n    %191 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc104)\n    %192 = stablehlo.constant dense<-3.000000e-04> : tensor<64x10xf32> loc(#loc154)\n    %193 = stablehlo.multiply %190, %192 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc154)\n    %194 = stablehlo.add %arg4, %193 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc155)\n    %195 = stablehlo.log %36 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc157)\n    %196 = stablehlo.broadcast_in_dim %195, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc127)\n    %197 = stablehlo.subtract %33, %196 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc127)\n    %198 = stablehlo.select %9, %197, %13 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xi1>, tensor<100x10xf32> loc(#loc121)\n    %199 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc106)\n    %200 = stablehlo.reduce(%198 init: %199) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %201 = stablehlo.negate %200 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc122)\n    %202 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc107)\n    %203 = stablehlo.reduce(%201 init: %202) across dimensions = [0] {mhlo.sharding = "{replicated}"} : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc4]), %arg17: tensor<f32> loc(fused[#loc6, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc113)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc113)\n    %204 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.00999999977> : tensor<f32> loc(#loc108)\n    %205 = stablehlo.multiply %203, %204 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc129)\n    %206 = stablehlo.tuple %1, %94, %127, %161, %194, %62, %54, %102, %136, %169, %74, %111, %145, %178, %205 {mhlo.sharding = "{{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=5*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=10*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}}", result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>], xla_shape = "(s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{1,0}, /*index=5*/s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, /*index=10*/f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, f32[])"} : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc109)\n    return %206 : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc)\n  } loc(#loc)\n} loc(#loc)\n#loc8 = loc("constant.5")\n#loc9 = loc("parallelize(train_step_shard_parallel)/jit(main)/add")\n#loc10 = loc("/usr/local/lib/python3.9/dist-packages/flax/training/train_state.py":77:0)\n#loc11 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc12 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/conv.py":212:0)\n#loc13 = loc("constant.73")\n#loc14 = loc("parallelize(train_step_shard_parallel)/jit(main)/max")\n#loc15 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/activate.py":25:0)\n#loc16 = loc("parallelize(train_step_shard_parallel)/jit(main)/gt")\n#loc17 = loc("parallelize(train_step_shard_parallel)/jit(main)/eq")\n#loc18 = loc("/data/hejing/distri/alpa/vgg16-alpa.py":123:0)\n#loc19 = loc("constant.77")\n#loc20 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 10) broadcast_dimensions=(0,)]")\n#loc21 = loc("constant.12")\n#loc22 = loc("broadcast")\n#loc23 = loc("parallelize(train_step_shard_parallel)/jit(main)/mul")\n#loc24 = loc("parallelize(train_step_shard_parallel)/jit(main)/neg")\n#loc25 = loc("constant.10")\n#loc26 = loc("constant.6")\n#loc27 = loc("constant.1")\n#loc28 = loc("parallelize(train_step_shard_parallel)/jit(main)/div")\n#loc29 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc30 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":95:0)\n#loc31 = loc("constant.80")\n#loc32 = loc("constant.85")\n#loc33 = loc("parallelize(train_step_shard_parallel)/jit(main)/sub")\n#loc34 = loc("parallelize(train_step_shard_parallel)/jit(main)/exp")\n#loc35 = loc("constant.7")\n#loc36 = loc("parallelize(train_step_shard_parallel)/jit(main)/add_any")\n#loc37 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]")\n#loc38 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 32, 32, 64) broadcast_dimensions=(0, 3)]")\n#loc39 = loc("parallelize(train_step_shard_parallel)/jit(main)/select_n")\n#loc40 = loc("parallelize(train_step_shard_parallel)/jit(main)/rev[dimensions=(0, 1)]")\n#loc41 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(2, 3, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc42 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc43 = loc("constant.15")\n#loc44 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":84:0)\n#loc45 = loc("constant.20")\n#loc46 = loc("constant.24")\n#loc47 = loc("constant.22")\n#loc48 = loc("constant.83")\n#loc49 = loc("parallelize(train_step_shard_parallel)/jit(main)/lt")\n#loc50 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/numerics.py":118:0)\n#loc51 = loc("constant.82")\n#loc52 = loc("constant.26")\n#loc53 = loc("parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]")\n#loc54 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":118:0)\n#loc55 = loc("parallelize(train_step_shard_parallel)/jit(main)/pow")\n#loc56 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":122:0)\n#loc57 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":98:0)\n#loc58 = loc("constant.32")\n#loc59 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":107:0)\n#loc60 = loc("constant.38")\n#loc61 = loc("constant.79")\n#loc62 = loc("constant.40")\n#loc63 = loc("parallelize(train_step_shard_parallel)/jit(main)/sqrt")\n#loc64 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":348:0)\n#loc65 = loc("constant.46")\n#loc66 = loc("multiply.2")\n#loc67 = loc("constant.52")\n#loc68 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":770:0)\n#loc69 = loc("constant.58")\n#loc70 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":518:0)\n#loc71 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/update.py":43:0)\n#loc72 = loc("constant.14")\n#loc73 = loc("constant.18")\n#loc74 = loc("constant.30")\n#loc75 = loc("constant.36")\n#loc76 = loc("constant.44")\n#loc77 = loc("multiply.3")\n#loc78 = loc("constant.50")\n#loc79 = loc("constant.56")\n#loc80 = loc("constant.11")\n#loc81 = loc("constant.13")\n#loc82 = loc("broadcast.64")\n#loc83 = loc("constant.16")\n#loc84 = loc("broadcast.62")\n#loc85 = loc("constant.28")\n#loc86 = loc("broadcast.48")\n#loc87 = loc("constant.34")\n#loc88 = loc("broadcast.46")\n#loc89 = loc("constant.42")\n#loc90 = loc("broadcast.36")\n#loc91 = loc("multiply.4")\n#loc92 = loc("constant.48")\n#loc93 = loc("broadcast.28")\n#loc94 = loc("constant.54")\n#loc95 = loc("broadcast.20")\n#loc96 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc97 = loc("constant.59")\n#loc98 = loc("constant.57")\n#loc99 = loc("constant.43")\n#loc100 = loc("constant.41")\n#loc101 = loc("constant.33")\n#loc102 = loc("multiply.5")\n#loc103 = loc("constant.25")\n#loc104 = loc("constant.17")\n#loc105 = loc("parallelize(train_step_shard_parallel)/jit(main)/log")\n#loc106 = loc("constant.8")\n#loc107 = loc("constant.9")\n#loc108 = loc("constant.4")\n#loc109 = loc("tuple.267")\n#loc115 = loc(fused[#loc9, #loc10])\n#loc116 = loc(fused[#loc11, #loc12])\n#loc117 = loc(fused[#loc14, #loc15])\n#loc118 = loc(fused[#loc16, #loc15])\n#loc119 = loc(fused[#loc17, #loc18])\n#loc120 = loc(fused[#loc20, #loc4])\n#loc121 = loc(fused[#loc23, #loc4])\n#loc122 = loc(fused[#loc24, #loc4])\n#loc123 = loc(fused[#loc28, #loc2])\n#loc124 = loc(fused[#loc29, #loc30])\n#loc125 = loc(fused[#loc9, #loc7])\n#loc126 = loc(fused[#loc9, #loc4])\n#loc127 = loc(fused[#loc33, #loc4])\n#loc128 = loc(fused[#loc34, #loc4])\n#loc129 = loc(fused[#loc28, #loc4])\n#loc130 = loc(fused[#loc36, #loc4])\n#loc131 = loc(fused[#loc37, #loc30])\n#loc132 = loc(fused[#loc38, #loc2])\n#loc133 = loc(fused[#loc39, #loc15])\n#loc134 = loc(fused[#loc40, #loc12])\n#loc135 = loc(fused[#loc41, #loc12])\n#loc136 = loc(fused[#loc42, #loc12])\n#loc137 = loc(fused[#loc23, #loc44])\n#loc138 = loc(fused[#loc9, #loc44])\n#loc139 = loc(fused[#loc49, #loc50])\n#loc140 = loc(fused[#loc9, #loc50])\n#loc141 = loc(fused[#loc39, #loc50])\n#loc142 = loc(fused[#loc53, #loc54])\n#loc143 = loc(fused[#loc55, #loc54])\n#loc144 = loc(fused[#loc33, #loc54])\n#loc145 = loc(fused[#loc28, #loc56])\n#loc146 = loc(fused[#loc23, #loc57])\n#loc147 = loc(fused[#loc23, #loc59])\n#loc148 = loc(fused[#loc9, #loc59])\n#loc149 = loc(fused[#loc63, #loc64])\n#loc150 = loc(fused[#loc9, #loc64])\n#loc151 = loc(fused[#loc28, #loc64])\n#loc152 = loc(fused[#loc23, #loc68])\n#loc153 = loc(fused[#loc9, #loc68])\n#loc154 = loc(fused[#loc23, #loc70])\n#loc155 = loc(fused[#loc9, #loc71])\n#loc156 = loc(fused[#loc96, #loc30])\n#loc157 = loc(fused[#loc105, #loc4])\n'
# print(sss2)

# print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
# sss3='#loc0 = loc(unknown)\n#loc1 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc2 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc3 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc4 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc5 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0])\nmodule @train_step_shard_parallel {\n  func.func private @region_0.93(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc1)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_1.117(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.maximum %arg0, %arg1 : tensor<f32> loc(#loc2)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_2.128(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_3.140(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_4.146(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc4)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_5.155(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_6.162(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_7.170(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc5)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func @main(%arg0: tensor<i32> loc(unknown), %arg1: tensor<3x3x3x32xf32> loc(unknown), %arg2: tensor<3x3x32x64xf32> loc(unknown), %arg3: tensor<10xf32> loc(unknown), %arg4: tensor<64x10xf32> loc(unknown), %arg5: tensor<i32> loc(unknown), %arg6: tensor<3x3x3x32xf32> loc(unknown), %arg7: tensor<3x3x32x64xf32> loc(unknown), %arg8: tensor<10xf32> loc(unknown), %arg9: tensor<64x10xf32> loc(unknown), %arg10: tensor<3x3x3x32xf32> loc(unknown), %arg11: tensor<3x3x32x64xf32> loc(unknown), %arg12: tensor<10xf32> loc(unknown), %arg13: tensor<64x10xf32> loc(unknown), %arg14: tensor<100x32x32x3xf32> loc(unknown), %arg15: tensor<100xi32> loc(unknown)) -> tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> {\n    %0 = mhlo.constant dense<1> : tensor<i32> loc(#loc6)\n    %1 = mhlo.add %arg0, %0 : tensor<i32> loc(#loc7)\n    %2 = mhlo.convolution(%arg14, %arg1) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<3x3x3x32xf32>) -> tensor<100x32x32x32xf32> loc(#loc8)\n    %3 = mhlo.convolution(%2, %arg2) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc9)\n    %4 = mhlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc10)\n    %5 = "mhlo.broadcast_in_dim"(%4) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<100x32x32x64xf32> loc(#loc11)\n    %6 = mhlo.compare  GT, %3, %5 : (tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xi1> loc(#loc12)\n    %7 = mhlo.reshape %arg15 : (tensor<100xi32>) -> tensor<100x1xi32> loc(#loc13)\n    %8 = "mhlo.broadcast_in_dim"(%7) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<100x1xi32>) -> tensor<100x1xi32> loc(#loc14)\n    %9 = mhlo.reshape %8 : (tensor<100x1xi32>) -> tensor<100xi32> loc(#loc14)\n    %10 = "mhlo.broadcast_in_dim"(%9) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<100xi32>) -> tensor<100x10xi32> loc(#loc14)\n    %11 = "mhlo.iota"() {iota_dimension = 0 : i64} : () -> tensor<10xi32> loc(#loc15)\n    %12 = mhlo.reshape %11 : (tensor<10xi32>) -> tensor<1x10xi32> loc(#loc15)\n    %13 = "mhlo.broadcast_in_dim"(%12) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<1x10xi32>) -> tensor<1x10xi32> loc(#loc14)\n    %14 = mhlo.reshape %13 : (tensor<1x10xi32>) -> tensor<10xi32> loc(#loc14)\n    %15 = "mhlo.broadcast_in_dim"(%14) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<10xi32>) -> tensor<100x10xi32> loc(#loc14)\n    %16 = mhlo.compare  EQ, %10, %15 : (tensor<100x10xi32>, tensor<100x10xi32>) -> tensor<100x10xi1> loc(#loc14)\n    %17 = mhlo.convert(%16) : (tensor<100x10xi1>) -> tensor<100x10xf32> loc(#loc16)\n    %18 = mhlo.constant dense<-0.00999999977> : tensor<f32> loc(#loc17)\n    %19 = "mhlo.broadcast_in_dim"(%18) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<100x10xf32> loc(#loc18)\n    %20 = mhlo.multiply %17, %19 : tensor<100x10xf32> loc(#loc19)\n    %21 = mhlo.negate %20 : tensor<100x10xf32> loc(#loc20)\n    %22 = mhlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc21)\n    %23 = mhlo.reduce(%21 init: %22) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %24 = mhlo.reshape %23 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc22)\n    %25 = mhlo.maximum %3, %5 : tensor<100x32x32x64xf32> loc(#loc11)\n    %26 = mhlo.reduce(%25 init: %22) across dimensions = [1, 2] : (tensor<100x32x32x64xf32>, tensor<f32>) -> tensor<100x64xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc1)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc1)\n    %27 = mhlo.constant dense<1.024000e+03> : tensor<f32> loc(#loc23)\n    %28 = "mhlo.broadcast_in_dim"(%27) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<100x64xf32> loc(#loc24)\n    %29 = mhlo.divide %26, %28 : tensor<100x64xf32> loc(#loc24)\n    %30 = "mhlo.dot"(%29, %arg4) {precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x64xf32>, tensor<64x10xf32>) -> tensor<100x10xf32> loc(#loc25)\n    %31 = mhlo.reshape %arg3 : (tensor<10xf32>) -> tensor<1x10xf32> loc(#loc26)\n    %32 = "mhlo.broadcast_in_dim"(%31) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<1x10xf32>) -> tensor<1x10xf32> loc(#loc27)\n    %33 = mhlo.reshape %32 : (tensor<1x10xf32>) -> tensor<10xf32> loc(#loc27)\n    %34 = "mhlo.broadcast_in_dim"(%33) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<10xf32>) -> tensor<100x10xf32> loc(#loc27)\n    %35 = mhlo.add %30, %34 : tensor<100x10xf32> loc(#loc27)\n    %36 = mhlo.constant dense<1.000000e-07> : tensor<f32> loc(#loc28)\n    %37 = "mhlo.broadcast_in_dim"(%36) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<100x10xf32> loc(#loc29)\n    %38 = mhlo.add %35, %37 : tensor<100x10xf32> loc(#loc29)\n    %39 = mhlo.constant dense<0xFF800000> : tensor<f32> loc(#loc30)\n    %40 = mhlo.reduce(%38 init: %39) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.maximum %arg16, %arg17 : tensor<f32> loc(#loc2)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc2)\n    %41 = mhlo.reshape %40 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc31)\n    %42 = "mhlo.broadcast_in_dim"(%41) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<100x1xf32>) -> tensor<100x1xf32> loc(#loc32)\n    %43 = mhlo.reshape %42 : (tensor<100x1xf32>) -> tensor<100xf32> loc(#loc32)\n    %44 = "mhlo.broadcast_in_dim"(%43) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc32)\n    %45 = mhlo.subtract %38, %44 : tensor<100x10xf32> loc(#loc32)\n    %46 = mhlo.exponential %45 : tensor<100x10xf32> loc(#loc33)\n    %47 = mhlo.reduce(%46 init: %22) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %48 = mhlo.reshape %47 : (tensor<100xf32>) -> tensor<100x1xf32> loc(#loc31)\n    %49 = mhlo.divide %24, %48 : tensor<100x1xf32> loc(#loc34)\n    %50 = mhlo.reduce(%49 init: %22) across dimensions = [1] : (tensor<100x1xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %51 = "mhlo.broadcast_in_dim"(%50) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc18)\n    %52 = mhlo.multiply %51, %46 : tensor<100x10xf32> loc(#loc19)\n    %53 = mhlo.add %20, %52 : tensor<100x10xf32> loc(#loc35)\n    %54 = "mhlo.dot_general"(%53, %arg4) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [1]>, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x10xf32>, tensor<64x10xf32>) -> tensor<100x64xf32> loc(#loc36)\n    %55 = mhlo.divide %54, %28 : tensor<100x64xf32> loc(#loc24)\n    %56 = "mhlo.broadcast_in_dim"(%55) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>} : (tensor<100x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc37)\n    %57 = "mhlo.select"(%6, %56, %5) : (tensor<100x32x32x64xi1>, tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc38)\n    %58 = "mhlo.reverse"(%arg2) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<3x3x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc39)\n    %59 = mhlo.convolution(%57, %58) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x64xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x32xf32> loc(#loc40)\n    %60 = mhlo.convolution(%arg14, %59) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<100x32x32x32xf32>) -> tensor<3x3x3x32xf32> loc(#loc41)\n    %61 = mhlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc42)\n    %62 = "mhlo.broadcast_in_dim"(%61) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc43)\n    %63 = mhlo.multiply %60, %62 : tensor<3x3x3x32xf32> loc(#loc43)\n    %64 = mhlo.constant dense<0.899999976> : tensor<f32> loc(#loc44)\n    %65 = "mhlo.broadcast_in_dim"(%64) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc43)\n    %66 = mhlo.multiply %arg6, %65 : tensor<3x3x3x32xf32> loc(#loc43)\n    %67 = mhlo.add %63, %66 : tensor<3x3x3x32xf32> loc(#loc45)\n    %68 = mhlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc46)\n    %69 = mhlo.constant dense<0.899999976> : tensor<f32> loc(#loc47)\n    %70 = mhlo.constant dense<2147483647> : tensor<i32> loc(#loc48)\n    %71 = mhlo.compare  LT, %arg5, %70 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc49)\n    %72 = mhlo.add %arg5, %0 : tensor<i32> loc(#loc50)\n    %73 = "mhlo.select"(%71, %72, %70) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32> loc(#loc51)\n    %74 = mhlo.convert(%73) : (tensor<i32>) -> tensor<f32> loc(#loc52)\n    %75 = mhlo.power %69, %74 : tensor<f32> loc(#loc53)\n    %76 = mhlo.subtract %68, %75 : tensor<f32> loc(#loc54)\n    %77 = "mhlo.broadcast_in_dim"(%76) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc55)\n    %78 = mhlo.divide %67, %77 : tensor<3x3x3x32xf32> loc(#loc55)\n    %79 = mhlo.multiply %60, %60 : tensor<3x3x3x32xf32> loc(#loc56)\n    %80 = mhlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc57)\n    %81 = "mhlo.broadcast_in_dim"(%80) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc58)\n    %82 = mhlo.multiply %79, %81 : tensor<3x3x3x32xf32> loc(#loc58)\n    %83 = mhlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc59)\n    %84 = "mhlo.broadcast_in_dim"(%83) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc58)\n    %85 = mhlo.multiply %arg10, %84 : tensor<3x3x3x32xf32> loc(#loc58)\n    %86 = mhlo.add %82, %85 : tensor<3x3x3x32xf32> loc(#loc60)\n    %87 = mhlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc61)\n    %88 = mhlo.convert(%73) : (tensor<i32>) -> tensor<f32> loc(#loc52)\n    %89 = mhlo.power %87, %88 : tensor<f32> loc(#loc53)\n    %90 = mhlo.subtract %68, %89 : tensor<f32> loc(#loc54)\n    %91 = "mhlo.broadcast_in_dim"(%90) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc55)\n    %92 = mhlo.divide %86, %91 : tensor<3x3x3x32xf32> loc(#loc55)\n    %93 = mhlo.sqrt %92 : tensor<3x3x3x32xf32> loc(#loc62)\n    %94 = mhlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc63)\n    %95 = "mhlo.broadcast_in_dim"(%94) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc64)\n    %96 = mhlo.add %93, %95 : tensor<3x3x3x32xf32> loc(#loc64)\n    %97 = mhlo.divide %78, %96 : tensor<3x3x3x32xf32> loc(#loc65)\n    %98 = mhlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc66)\n    %99 = "mhlo.broadcast_in_dim"(%98) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc67)\n    %100 = mhlo.multiply %arg1, %99 : tensor<3x3x3x32xf32> loc(#loc67)\n    %101 = mhlo.add %97, %100 : tensor<3x3x3x32xf32> loc(#loc68)\n    %102 = mhlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc69)\n    %103 = "mhlo.broadcast_in_dim"(%102) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc70)\n    %104 = mhlo.multiply %101, %103 : tensor<3x3x3x32xf32> loc(#loc70)\n    %105 = mhlo.add %arg1, %104 : tensor<3x3x3x32xf32> loc(#loc71)\n    %106 = mhlo.convolution(%2, %57) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<100x32x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc72)\n    %107 = mhlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc73)\n    %108 = "mhlo.broadcast_in_dim"(%107) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc43)\n    %109 = mhlo.multiply %106, %108 : tensor<3x3x32x64xf32> loc(#loc43)\n    %110 = mhlo.constant dense<0.899999976> : tensor<f32> loc(#loc74)\n    %111 = "mhlo.broadcast_in_dim"(%110) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc43)\n    %112 = mhlo.multiply %arg7, %111 : tensor<3x3x32x64xf32> loc(#loc43)\n    %113 = mhlo.add %109, %112 : tensor<3x3x32x64xf32> loc(#loc45)\n    %114 = "mhlo.broadcast_in_dim"(%76) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc55)\n    %115 = mhlo.divide %113, %114 : tensor<3x3x32x64xf32> loc(#loc55)\n    %116 = mhlo.multiply %106, %106 : tensor<3x3x32x64xf32> loc(#loc56)\n    %117 = mhlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc75)\n    %118 = "mhlo.broadcast_in_dim"(%117) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc58)\n    %119 = mhlo.multiply %116, %118 : tensor<3x3x32x64xf32> loc(#loc58)\n    %120 = mhlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc76)\n    %121 = "mhlo.broadcast_in_dim"(%120) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc58)\n    %122 = mhlo.multiply %arg11, %121 : tensor<3x3x32x64xf32> loc(#loc58)\n    %123 = mhlo.add %119, %122 : tensor<3x3x32x64xf32> loc(#loc60)\n    %124 = "mhlo.broadcast_in_dim"(%90) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc55)\n    %125 = mhlo.divide %123, %124 : tensor<3x3x32x64xf32> loc(#loc55)\n    %126 = mhlo.sqrt %125 : tensor<3x3x32x64xf32> loc(#loc62)\n    %127 = mhlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc77)\n    %128 = "mhlo.broadcast_in_dim"(%127) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc64)\n    %129 = mhlo.add %126, %128 : tensor<3x3x32x64xf32> loc(#loc64)\n    %130 = mhlo.divide %115, %129 : tensor<3x3x32x64xf32> loc(#loc65)\n    %131 = mhlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc78)\n    %132 = "mhlo.broadcast_in_dim"(%131) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc67)\n    %133 = mhlo.multiply %arg2, %132 : tensor<3x3x32x64xf32> loc(#loc67)\n    %134 = mhlo.add %130, %133 : tensor<3x3x32x64xf32> loc(#loc68)\n    %135 = mhlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc79)\n    %136 = "mhlo.broadcast_in_dim"(%135) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc70)\n    %137 = mhlo.multiply %134, %136 : tensor<3x3x32x64xf32> loc(#loc70)\n    %138 = mhlo.add %arg2, %137 : tensor<3x3x32x64xf32> loc(#loc71)\n    %139 = mhlo.reduce(%53 init: %22) across dimensions = [0] : (tensor<100x10xf32>, tensor<f32>) -> tensor<10xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc5)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc5)\n    %140 = mhlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc80)\n    %141 = "mhlo.broadcast_in_dim"(%140) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc81)\n    %142 = mhlo.multiply %139, %141 : tensor<10xf32> loc(#loc43)\n    %143 = mhlo.constant dense<0.899999976> : tensor<f32> loc(#loc82)\n    %144 = "mhlo.broadcast_in_dim"(%143) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc83)\n    %145 = mhlo.multiply %arg8, %144 : tensor<10xf32> loc(#loc43)\n    %146 = mhlo.add %142, %145 : tensor<10xf32> loc(#loc45)\n    %147 = "mhlo.broadcast_in_dim"(%76) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc55)\n    %148 = mhlo.divide %146, %147 : tensor<10xf32> loc(#loc55)\n    %149 = mhlo.multiply %139, %139 : tensor<10xf32> loc(#loc56)\n    %150 = mhlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc84)\n    %151 = "mhlo.broadcast_in_dim"(%150) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc85)\n    %152 = mhlo.multiply %149, %151 : tensor<10xf32> loc(#loc58)\n    %153 = mhlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc86)\n    %154 = "mhlo.broadcast_in_dim"(%153) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc87)\n    %155 = mhlo.multiply %arg12, %154 : tensor<10xf32> loc(#loc58)\n    %156 = mhlo.add %152, %155 : tensor<10xf32> loc(#loc60)\n    %157 = "mhlo.broadcast_in_dim"(%90) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc55)\n    %158 = mhlo.divide %156, %157 : tensor<10xf32> loc(#loc55)\n    %159 = mhlo.sqrt %158 : tensor<10xf32> loc(#loc62)\n    %160 = mhlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc88)\n    %161 = "mhlo.broadcast_in_dim"(%160) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc89)\n    %162 = mhlo.add %159, %161 : tensor<10xf32> loc(#loc64)\n    %163 = mhlo.divide %148, %162 : tensor<10xf32> loc(#loc65)\n    %164 = mhlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc90)\n    %165 = "mhlo.broadcast_in_dim"(%164) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc91)\n    %166 = mhlo.multiply %arg3, %165 : tensor<10xf32> loc(#loc67)\n    %167 = mhlo.add %163, %166 : tensor<10xf32> loc(#loc68)\n    %168 = mhlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc92)\n    %169 = "mhlo.broadcast_in_dim"(%168) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<10xf32> loc(#loc93)\n    %170 = mhlo.multiply %167, %169 : tensor<10xf32> loc(#loc70)\n    %171 = mhlo.add %arg3, %170 : tensor<10xf32> loc(#loc71)\n    %172 = "mhlo.dot_general"(%53, %29) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [0], rhs_contracting_dimensions = [0]>, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x10xf32>, tensor<100x64xf32>) -> tensor<10x64xf32> loc(#loc94)\n    %173 = "mhlo.transpose"(%172) {permutation = dense<[1, 0]> : tensor<2xi64>, xla_shape = "f32[64,10]{0,1}"} : (tensor<10x64xf32>) -> tensor<64x10xf32> loc(#loc95)\n    %174 = mhlo.constant dense<1.000000e-01> : tensor<f32> loc(#loc96)\n    %175 = "mhlo.broadcast_in_dim"(%174) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc43)\n    %176 = mhlo.multiply %173, %175 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc43)\n    %177 = mhlo.constant dense<0.899999976> : tensor<f32> loc(#loc97)\n    %178 = "mhlo.broadcast_in_dim"(%177) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc43)\n    %179 = mhlo.multiply %arg9, %178 : tensor<64x10xf32> loc(#loc43)\n    %180 = mhlo.add %176, %179 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc45)\n    %181 = "mhlo.broadcast_in_dim"(%76) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc55)\n    %182 = mhlo.divide %180, %181 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc55)\n    %183 = mhlo.multiply %173, %173 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc56)\n    %184 = mhlo.constant dense<1.000000e-03> : tensor<f32> loc(#loc98)\n    %185 = "mhlo.broadcast_in_dim"(%184) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc58)\n    %186 = mhlo.multiply %183, %185 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc58)\n    %187 = mhlo.constant dense<9.990000e-01> : tensor<f32> loc(#loc99)\n    %188 = "mhlo.broadcast_in_dim"(%187) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc58)\n    %189 = mhlo.multiply %arg13, %188 : tensor<64x10xf32> loc(#loc58)\n    %190 = mhlo.add %186, %189 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc60)\n    %191 = "mhlo.broadcast_in_dim"(%90) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc55)\n    %192 = mhlo.divide %190, %191 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc55)\n    %193 = mhlo.sqrt %192 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc62)\n    %194 = mhlo.constant dense<9.99999993E-9> : tensor<f32> loc(#loc100)\n    %195 = "mhlo.broadcast_in_dim"(%194) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc64)\n    %196 = mhlo.add %193, %195 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc64)\n    %197 = mhlo.divide %182, %196 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc65)\n    %198 = mhlo.constant dense<5.000000e-04> : tensor<f32> loc(#loc101)\n    %199 = "mhlo.broadcast_in_dim"(%198) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc67)\n    %200 = mhlo.multiply %arg4, %199 : tensor<64x10xf32> loc(#loc67)\n    %201 = mhlo.add %197, %200 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc68)\n    %202 = mhlo.constant dense<-3.000000e-04> : tensor<f32> loc(#loc102)\n    %203 = "mhlo.broadcast_in_dim"(%202) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc70)\n    %204 = mhlo.multiply %201, %203 {xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc70)\n    %205 = mhlo.add %arg4, %204 : tensor<64x10xf32> loc(#loc71)\n    %206 = mhlo.log %48 : tensor<100x1xf32> loc(#loc103)\n    %207 = "mhlo.broadcast_in_dim"(%206) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<100x1xf32>) -> tensor<100x1xf32> loc(#loc32)\n    %208 = mhlo.reshape %207 : (tensor<100x1xf32>) -> tensor<100xf32> loc(#loc32)\n    %209 = "mhlo.broadcast_in_dim"(%208) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc32)\n    %210 = mhlo.subtract %45, %209 : tensor<100x10xf32> loc(#loc32)\n    %211 = mhlo.multiply %17, %210 : tensor<100x10xf32> loc(#loc19)\n    %212 = mhlo.reduce(%211 init: %22) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %213 = mhlo.negate %212 : tensor<100xf32> loc(#loc20)\n    %214 = mhlo.reduce(%213 init: %22) across dimensions = [0] : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %219 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc4)\n      mhlo.return %219 : tensor<f32> loc(#loc0)\n    } loc(#loc4)\n    %215 = mhlo.constant dense<1.000000e+02> : tensor<f32> loc(#loc104)\n    %216 = mhlo.divide %214, %215 : tensor<f32> loc(#loc34)\n    %217 = mhlo.divide %216, %68 : tensor<f32> loc(#loc105)\n    %218 = "mhlo.tuple"(%1, %105, %138, %171, %205, %73, %67, %113, %146, %180, %86, %123, %156, %190, %217) {xla_shape = "(s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{1,0}, /*index=5*/s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, /*index=10*/f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, f32[])"} : (tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>) -> tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc106)\n    return %218 : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc0)\n  } loc(#loc0)\n} loc(#loc0)\n#loc6 = loc("constant.82")\n#loc7 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/flax/training/train_state.py":77:0])\n#loc8 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 3) rhs_shape=(3, 3, 3, 32) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc9 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 32) rhs_shape=(3, 3, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc10 = loc("constant.73")\n#loc11 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/max", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc12 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/gt", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc13 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 1) broadcast_dimensions=(0,)]", "/data/czy/alpa/vgg16-alpa.py":123:0])\n#loc14 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/eq", "/data/czy/alpa/vgg16-alpa.py":123:0])\n#loc15 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/iota[dtype=int32 shape=(1, 10) dimension=1]", "/data/czy/alpa/vgg16-alpa.py":123:0])\n#loc16 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]", "/data/czy/alpa/vgg16-alpa.py":123:0])\n#loc17 = loc("constant.77")\n#loc18 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 10) broadcast_dimensions=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc19 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc20 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/neg", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc21 = loc("constant.88")\n#loc22 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reshape[new_sizes=(100, 1) dimensions=None]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc23 = loc("constant.75")\n#loc24 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc25 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc26 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reshape[new_sizes=(1, 10) dimensions=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0])\n#loc27 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0])\n#loc28 = loc("constant.80")\n#loc29 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc30 = loc("constant.87")\n#loc31 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 1) broadcast_dimensions=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc32 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sub", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc33 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/exp", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc34 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc35 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add_any", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc36 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc37 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 32, 32, 64) broadcast_dimensions=(0, 3)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc38 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/select_n", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc39 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/rev[dimensions=(0, 1)]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc40 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(2, 3, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 64) rhs_shape=(3, 3, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc41 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 3) rhs_shape=(100, 32, 32, 32) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc42 = loc("constant.71")\n#loc43 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":84:0])\n#loc44 = loc("constant.69")\n#loc45 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":84:0])\n#loc46 = loc("constant.79")\n#loc47 = loc("constant.85")\n#loc48 = loc("constant.83")\n#loc49 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/lt", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc50 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc51 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/select_n", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc52 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=True]", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc53 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/pow", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc54 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sub", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc55 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":122:0])\n#loc56 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":98:0])\n#loc57 = loc("constant.55")\n#loc58 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":107:0])\n#loc59 = loc("constant.53")\n#loc60 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":107:0])\n#loc61 = loc("constant.84")\n#loc62 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sqrt", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc63 = loc("constant.39")\n#loc64 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc65 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc66 = loc("constant.31")\n#loc67 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":770:0])\n#loc68 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":770:0])\n#loc69 = loc("constant.23")\n#loc70 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":518:0])\n#loc71 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/update.py":43:0])\n#loc72 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 32) rhs_shape=(100, 32, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc73 = loc("constant.67")\n#loc74 = loc("constant.65")\n#loc75 = loc("constant.51")\n#loc76 = loc("constant.49")\n#loc77 = loc("constant.37")\n#loc78 = loc("constant.29")\n#loc79 = loc("constant.21")\n#loc80 = loc("constant.63")\n#loc81 = loc("broadcast.64")\n#loc82 = loc("constant.61")\n#loc83 = loc("broadcast.62")\n#loc84 = loc("constant.47")\n#loc85 = loc("broadcast.48")\n#loc86 = loc("constant.45")\n#loc87 = loc("broadcast.46")\n#loc88 = loc("constant.35")\n#loc89 = loc("broadcast.36")\n#loc90 = loc("constant.27")\n#loc91 = loc("broadcast.28")\n#loc92 = loc("constant.19")\n#loc93 = loc("broadcast.20")\n#loc94 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc95 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/transpose[permutation=(1, 0)]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc96 = loc("constant.59")\n#loc97 = loc("constant.57")\n#loc98 = loc("constant.43")\n#loc99 = loc("constant.41")\n#loc100 = loc("constant.33")\n#loc101 = loc("constant.25")\n#loc102 = loc("constant.17")\n#loc103 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/log", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc104 = loc("constant.86")\n#loc105 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/data/czy/alpa/vgg16-alpa.py":124:0])\n#loc106 = loc("tuple.267")\n'
# print(sss3)
# print('-----------------------------------------------------------')
# sss4='#loc0 = loc(unknown)\n#loc1 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc2 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc3 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc4 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc5 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0])\nmodule @train_step_shard_parallel {\n  func.func private @region_0.93(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc1)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_1.117(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.maximum %arg0, %arg1 : tensor<f32> loc(#loc2)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_2.128(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_3.140(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_4.146(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc4)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_5.155(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc3)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func private @region_7.170(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = mhlo.add %arg0, %arg1 : tensor<f32> loc(#loc5)\n    return %0 : tensor<f32> loc(#loc0)\n  } loc(#loc0)\n  func.func @main(%arg0: tensor<i32> {mhlo.sharding = ""} loc(unknown), %arg1: tensor<3x3x3x32xf32> {mhlo.sharding = ""} loc(unknown), %arg2: tensor<3x3x32x64xf32> {mhlo.sharding = ""} loc(unknown), %arg3: tensor<10xf32> {mhlo.sharding = ""} loc(unknown), %arg4: tensor<64x10xf32> {mhlo.sharding = ""} loc(unknown), %arg5: tensor<i32> {mhlo.sharding = ""} loc(unknown), %arg6: tensor<3x3x3x32xf32> {mhlo.sharding = ""} loc(unknown), %arg7: tensor<3x3x32x64xf32> {mhlo.sharding = ""} loc(unknown), %arg8: tensor<10xf32> {mhlo.sharding = ""} loc(unknown), %arg9: tensor<64x10xf32> {mhlo.sharding = ""} loc(unknown), %arg10: tensor<3x3x3x32xf32> {mhlo.sharding = ""} loc(unknown), %arg11: tensor<3x3x32x64xf32> {mhlo.sharding = ""} loc(unknown), %arg12: tensor<10xf32> {mhlo.sharding = ""} loc(unknown), %arg13: tensor<64x10xf32> {mhlo.sharding = ""} loc(unknown), %arg14: tensor<100x32x32x3xf32> {mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01"} loc(unknown), %arg15: tensor<100xi32> {mhlo.sharding = "\\08\\03\\1A\\01\\02\\22\\02\\00\\01"} loc(unknown)) -> (tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> {mhlo.sharding = "\\08\\02*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00"}) {\n    %0 = mhlo.constant {mhlo.sharding = ""} dense<1> : tensor<i32> loc(#loc6)\n    %1 = mhlo.add %arg0, %0 {mhlo.sharding = ""} : tensor<i32> loc(#loc7)\n    %2 = mhlo.convolution(%arg14, %arg1) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<3x3x3x32xf32>) -> tensor<100x32x32x32xf32> loc(#loc8)\n    %3 = mhlo.convolution(%2, %arg2) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc9)\n    %4 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc10)\n    %5 = "mhlo.broadcast_in_dim"(%4) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<100x32x32x64xf32> loc(#loc11)\n    %6 = mhlo.compare  GT, %3, %5 {mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01"} : (tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xi1> loc(#loc12)\n    %7 = "mhlo.broadcast_in_dim"(%arg15) {broadcast_dimensions = dense<0> : tensor<1xi64>, mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100xi32>) -> tensor<100x10xi32> loc(#loc13)\n    %8 = "mhlo.iota"() {iota_dimension = 1 : i64} : () -> tensor<100x10xi32> loc(#loc13)\n    %9 = mhlo.compare  EQ, %7, %8 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100x10xi32>, tensor<100x10xi32>) -> tensor<100x10xi1> loc(#loc13)\n    %10 = mhlo.constant {mhlo.sharding = ""} dense<-0.00999999977> : tensor<f32> loc(#loc14)\n    %11 = "mhlo.broadcast_in_dim"(%10) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<100x10xf32> loc(#loc15)\n    %12 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc16)\n    %13 = "mhlo.broadcast_in_dim"(%12) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<100x10xf32> loc(#loc17)\n    %14 = "mhlo.select"(%9, %11, %13) {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100x10xi1>, tensor<100x10xf32>, tensor<100x10xf32>) -> tensor<100x10xf32> loc(#loc18)\n    %15 = mhlo.negate %14 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc19)\n    %16 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc20)\n    %17 = mhlo.reduce(%15 init: %16) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %18 = mhlo.maximum %3, %5 {mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01"} : tensor<100x32x32x64xf32> loc(#loc11)\n    %19 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc21)\n    %20 = mhlo.reduce(%18 init: %19) across dimensions = [1, 2] : (tensor<100x32x32x64xf32>, tensor<f32>) -> tensor<100x64xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc1)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc1)\n    %21 = mhlo.constant {mhlo.sharding = ""} dense<9.765625E-4> : tensor<f32> loc(#loc22)\n    %22 = "mhlo.broadcast_in_dim"(%21) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<100x64xf32> loc(#loc23)\n    %23 = mhlo.multiply %20, %22 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x64xf32> loc(#loc23)\n    %24 = "mhlo.dot"(%23, %arg4) {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x64xf32>, tensor<64x10xf32>) -> tensor<100x10xf32> loc(#loc24)\n    %25 = "mhlo.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense<1> : tensor<1xi64>, mhlo.sharding = ""} : (tensor<10xf32>) -> tensor<100x10xf32> loc(#loc25)\n    %26 = mhlo.add %24, %25 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc25)\n    %27 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-07> : tensor<f32> loc(#loc26)\n    %28 = "mhlo.broadcast_in_dim"(%27) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<100x10xf32> loc(#loc27)\n    %29 = mhlo.add %26, %28 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc27)\n    %30 = mhlo.constant {mhlo.sharding = ""} dense<0xFF800000> : tensor<f32> loc(#loc28)\n    %31 = mhlo.reduce(%29 init: %30) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %207 = mhlo.maximum %arg16, %arg17 : tensor<f32> loc(#loc2)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc2)\n    %32 = "mhlo.broadcast_in_dim"(%31) {broadcast_dimensions = dense<0> : tensor<1xi64>, mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc29)\n    %33 = mhlo.subtract %29, %32 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc29)\n    %34 = mhlo.exponential %33 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc30)\n    %35 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc31)\n    %36 = mhlo.reduce(%34 init: %35) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %37 = mhlo.divide %17, %36 {mhlo.sharding = "\\08\\03\\1A\\01\\02\\22\\02\\00\\01"} : tensor<100xf32> loc(#loc32)\n    %38 = "mhlo.broadcast_in_dim"(%37) {broadcast_dimensions = dense<0> : tensor<1xi64>, mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc15)\n    %39 = mhlo.multiply %38, %34 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc18)\n    %40 = mhlo.add %14, %39 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc33)\n    %41 = "mhlo.dot_general"(%40, %arg4) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [1]>, mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x10xf32>, tensor<64x10xf32>) -> tensor<100x64xf32> loc(#loc34)\n    %42 = mhlo.multiply %41, %22 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x64xf32> loc(#loc23)\n    %43 = "mhlo.broadcast_in_dim"(%42) {broadcast_dimensions = dense<[0, 3]> : tensor<2xi64>, mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01"} : (tensor<100x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc35)\n    %44 = "mhlo.select"(%6, %43, %5) {mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01"} : (tensor<100x32x32x64xi1>, tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc36)\n    %45 = "mhlo.reverse"(%arg2) {dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<3x3x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc37)\n    %46 = mhlo.convolution(%44, %45) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "\\08\\03\\1A\\04\\02\\01\\01\\01\\22\\02\\00\\01", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x64xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x32xf32> loc(#loc38)\n    %47 = mhlo.convolution(%arg14, %46) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<100x32x32x32xf32>) -> tensor<3x3x3x32xf32> loc(#loc39)\n    %48 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-01> : tensor<f32> loc(#loc40)\n    %49 = "mhlo.broadcast_in_dim"(%48) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc41)\n    %50 = mhlo.multiply %47, %49 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc41)\n    %51 = mhlo.constant {mhlo.sharding = ""} dense<0.899999976> : tensor<f32> loc(#loc42)\n    %52 = "mhlo.broadcast_in_dim"(%51) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc41)\n    %53 = mhlo.multiply %arg6, %52 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc41)\n    %54 = mhlo.add %50, %53 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc43)\n    %55 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e+00> : tensor<f32> loc(#loc44)\n    %56 = mhlo.constant {mhlo.sharding = ""} dense<0.899999976> : tensor<f32> loc(#loc45)\n    %57 = mhlo.constant {mhlo.sharding = ""} dense<2147483647> : tensor<i32> loc(#loc46)\n    %58 = mhlo.compare  LT, %arg5, %57 {mhlo.sharding = ""} : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc47)\n    %59 = mhlo.constant {mhlo.sharding = ""} dense<1> : tensor<i32> loc(#loc48)\n    %60 = mhlo.add %arg5, %59 {mhlo.sharding = ""} : tensor<i32> loc(#loc49)\n    %61 = mhlo.constant {mhlo.sharding = ""} dense<2147483647> : tensor<i32> loc(#loc50)\n    %62 = "mhlo.select"(%58, %60, %61) {mhlo.sharding = ""} : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32> loc(#loc51)\n    %63 = mhlo.convert(%62) {mhlo.sharding = ""} : (tensor<i32>) -> tensor<f32> loc(#loc52)\n    %64 = mhlo.power %56, %63 {mhlo.sharding = ""} : tensor<f32> loc(#loc53)\n    %65 = mhlo.subtract %55, %64 {mhlo.sharding = ""} : tensor<f32> loc(#loc54)\n    %66 = "mhlo.broadcast_in_dim"(%65) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc55)\n    %67 = mhlo.multiply %47, %47 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc56)\n    %68 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-03> : tensor<f32> loc(#loc57)\n    %69 = "mhlo.broadcast_in_dim"(%68) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc58)\n    %70 = mhlo.multiply %67, %69 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc58)\n    %71 = mhlo.constant {mhlo.sharding = ""} dense<9.990000e-01> : tensor<f32> loc(#loc59)\n    %72 = "mhlo.broadcast_in_dim"(%71) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc58)\n    %73 = mhlo.multiply %arg10, %72 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc58)\n    %74 = mhlo.add %70, %73 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc60)\n    %75 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e+00> : tensor<f32> loc(#loc61)\n    %76 = mhlo.constant {mhlo.sharding = ""} dense<9.990000e-01> : tensor<f32> loc(#loc62)\n    %77 = mhlo.power %76, %63 {mhlo.sharding = ""} : tensor<f32> loc(#loc53)\n    %78 = mhlo.subtract %75, %77 {mhlo.sharding = ""} : tensor<f32> loc(#loc54)\n    %79 = "mhlo.broadcast_in_dim"(%78) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc55)\n    %80 = mhlo.divide %74, %79 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc55)\n    %81 = mhlo.sqrt %80 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc63)\n    %82 = mhlo.constant {mhlo.sharding = ""} dense<9.99999993E-9> : tensor<f32> loc(#loc64)\n    %83 = "mhlo.broadcast_in_dim"(%82) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc65)\n    %84 = mhlo.add %81, %83 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc65)\n    %85 = mhlo.multiply %66, %84 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc66)\n    %86 = mhlo.divide %54, %85 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc67)\n    %87 = mhlo.constant {mhlo.sharding = ""} dense<5.000000e-04> : tensor<f32> loc(#loc68)\n    %88 = "mhlo.broadcast_in_dim"(%87) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc69)\n    %89 = mhlo.multiply %arg1, %88 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc69)\n    %90 = mhlo.add %86, %89 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc70)\n    %91 = mhlo.constant {mhlo.sharding = ""} dense<-3.000000e-04> : tensor<f32> loc(#loc71)\n    %92 = "mhlo.broadcast_in_dim"(%91) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc72)\n    %93 = mhlo.multiply %90, %92 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc72)\n    %94 = mhlo.add %arg1, %93 {mhlo.sharding = ""} : tensor<3x3x3x32xf32> loc(#loc73)\n    %95 = mhlo.convolution(%2, %44) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<100x32x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc74)\n    %96 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-01> : tensor<f32> loc(#loc75)\n    %97 = "mhlo.broadcast_in_dim"(%96) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc41)\n    %98 = mhlo.multiply %95, %97 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc41)\n    %99 = mhlo.constant {mhlo.sharding = ""} dense<0.899999976> : tensor<f32> loc(#loc76)\n    %100 = "mhlo.broadcast_in_dim"(%99) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc41)\n    %101 = mhlo.multiply %arg7, %100 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc41)\n    %102 = mhlo.add %98, %101 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc43)\n    %103 = "mhlo.broadcast_in_dim"(%65) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc55)\n    %104 = mhlo.multiply %95, %95 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc56)\n    %105 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-03> : tensor<f32> loc(#loc77)\n    %106 = "mhlo.broadcast_in_dim"(%105) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc58)\n    %107 = mhlo.multiply %104, %106 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc58)\n    %108 = mhlo.constant {mhlo.sharding = ""} dense<9.990000e-01> : tensor<f32> loc(#loc78)\n    %109 = "mhlo.broadcast_in_dim"(%108) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc58)\n    %110 = mhlo.multiply %arg11, %109 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc58)\n    %111 = mhlo.add %107, %110 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc60)\n    %112 = "mhlo.broadcast_in_dim"(%78) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc55)\n    %113 = mhlo.divide %111, %112 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc55)\n    %114 = mhlo.sqrt %113 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc63)\n    %115 = mhlo.constant {mhlo.sharding = ""} dense<9.99999993E-9> : tensor<f32> loc(#loc79)\n    %116 = "mhlo.broadcast_in_dim"(%115) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc65)\n    %117 = mhlo.add %114, %116 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc65)\n    %118 = mhlo.multiply %103, %117 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc80)\n    %119 = mhlo.divide %102, %118 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc67)\n    %120 = mhlo.constant {mhlo.sharding = ""} dense<5.000000e-04> : tensor<f32> loc(#loc81)\n    %121 = "mhlo.broadcast_in_dim"(%120) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc69)\n    %122 = mhlo.multiply %arg2, %121 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc69)\n    %123 = mhlo.add %119, %122 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc70)\n    %124 = mhlo.constant {mhlo.sharding = ""} dense<-3.000000e-04> : tensor<f32> loc(#loc82)\n    %125 = "mhlo.broadcast_in_dim"(%124) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc72)\n    %126 = mhlo.multiply %123, %125 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc72)\n    %127 = mhlo.add %arg2, %126 {mhlo.sharding = ""} : tensor<3x3x32x64xf32> loc(#loc73)\n    %128 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc83)\n    %129 = mhlo.reduce(%40 init: %128) across dimensions = [0] : (tensor<100x10xf32>, tensor<f32>) -> tensor<10xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc5)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc5)\n    %130 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-01> : tensor<f32> loc(#loc84)\n    %131 = "mhlo.broadcast_in_dim"(%130) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc85)\n    %132 = mhlo.multiply %129, %131 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc41)\n    %133 = mhlo.constant {mhlo.sharding = ""} dense<0.899999976> : tensor<f32> loc(#loc86)\n    %134 = "mhlo.broadcast_in_dim"(%133) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc87)\n    %135 = mhlo.multiply %arg8, %134 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc41)\n    %136 = mhlo.add %132, %135 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc43)\n    %137 = "mhlo.broadcast_in_dim"(%65) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc55)\n    %138 = mhlo.multiply %129, %129 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc56)\n    %139 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-03> : tensor<f32> loc(#loc88)\n    %140 = "mhlo.broadcast_in_dim"(%139) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc89)\n    %141 = mhlo.multiply %138, %140 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc58)\n    %142 = mhlo.constant {mhlo.sharding = ""} dense<9.990000e-01> : tensor<f32> loc(#loc90)\n    %143 = "mhlo.broadcast_in_dim"(%142) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc91)\n    %144 = mhlo.multiply %arg12, %143 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc58)\n    %145 = mhlo.add %141, %144 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc60)\n    %146 = "mhlo.broadcast_in_dim"(%78) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc55)\n    %147 = mhlo.divide %145, %146 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc55)\n    %148 = mhlo.sqrt %147 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc63)\n    %149 = mhlo.constant {mhlo.sharding = ""} dense<9.99999993E-9> : tensor<f32> loc(#loc92)\n    %150 = "mhlo.broadcast_in_dim"(%149) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc93)\n    %151 = mhlo.add %148, %150 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc65)\n    %152 = mhlo.multiply %137, %151 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc94)\n    %153 = mhlo.divide %136, %152 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc67)\n    %154 = mhlo.constant {mhlo.sharding = ""} dense<5.000000e-04> : tensor<f32> loc(#loc95)\n    %155 = "mhlo.broadcast_in_dim"(%154) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc96)\n    %156 = mhlo.multiply %arg3, %155 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc69)\n    %157 = mhlo.add %153, %156 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc70)\n    %158 = mhlo.constant {mhlo.sharding = ""} dense<-3.000000e-04> : tensor<f32> loc(#loc97)\n    %159 = "mhlo.broadcast_in_dim"(%158) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<10xf32> loc(#loc98)\n    %160 = mhlo.multiply %157, %159 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc72)\n    %161 = mhlo.add %arg3, %160 {mhlo.sharding = ""} : tensor<10xf32> loc(#loc73)\n    %162 = "mhlo.dot_general"(%23, %40) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [0], rhs_contracting_dimensions = [0]>, mhlo.sharding = "", precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>], xla_shape = "f32[64,10]{0,1}"} : (tensor<100x64xf32>, tensor<100x10xf32>) -> tensor<64x10xf32> loc(#loc99)\n    %163 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-01> : tensor<f32> loc(#loc100)\n    %164 = "mhlo.broadcast_in_dim"(%163) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc41)\n    %165 = mhlo.multiply %162, %164 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc41)\n    %166 = mhlo.constant {mhlo.sharding = ""} dense<0.899999976> : tensor<f32> loc(#loc101)\n    %167 = "mhlo.broadcast_in_dim"(%166) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc41)\n    %168 = mhlo.multiply %arg9, %167 {mhlo.sharding = ""} : tensor<64x10xf32> loc(#loc41)\n    %169 = mhlo.add %165, %168 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc43)\n    %170 = "mhlo.broadcast_in_dim"(%65) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc55)\n    %171 = mhlo.multiply %162, %162 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc56)\n    %172 = mhlo.constant {mhlo.sharding = ""} dense<1.000000e-03> : tensor<f32> loc(#loc102)\n    %173 = "mhlo.broadcast_in_dim"(%172) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc58)\n    %174 = mhlo.multiply %171, %173 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc58)\n    %175 = mhlo.constant {mhlo.sharding = ""} dense<9.990000e-01> : tensor<f32> loc(#loc103)\n    %176 = "mhlo.broadcast_in_dim"(%175) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc58)\n    %177 = mhlo.multiply %arg13, %176 {mhlo.sharding = ""} : tensor<64x10xf32> loc(#loc58)\n    %178 = mhlo.add %174, %177 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc60)\n    %179 = "mhlo.broadcast_in_dim"(%78) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc55)\n    %180 = mhlo.divide %178, %179 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc55)\n    %181 = mhlo.sqrt %180 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc63)\n    %182 = mhlo.constant {mhlo.sharding = ""} dense<9.99999993E-9> : tensor<f32> loc(#loc104)\n    %183 = "mhlo.broadcast_in_dim"(%182) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc65)\n    %184 = mhlo.add %181, %183 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc65)\n    %185 = mhlo.multiply %170, %184 {mhlo.sharding = ""} : tensor<64x10xf32> loc(#loc105)\n    %186 = mhlo.divide %169, %185 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc67)\n    %187 = mhlo.constant {mhlo.sharding = ""} dense<5.000000e-04> : tensor<f32> loc(#loc106)\n    %188 = "mhlo.broadcast_in_dim"(%187) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc69)\n    %189 = mhlo.multiply %arg4, %188 {mhlo.sharding = ""} : tensor<64x10xf32> loc(#loc69)\n    %190 = mhlo.add %186, %189 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc70)\n    %191 = mhlo.constant {mhlo.sharding = ""} dense<-3.000000e-04> : tensor<f32> loc(#loc107)\n    %192 = "mhlo.broadcast_in_dim"(%191) {broadcast_dimensions = dense<> : tensor<0xi64>, mhlo.sharding = ""} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc72)\n    %193 = mhlo.multiply %190, %192 {mhlo.sharding = "", xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc72)\n    %194 = mhlo.add %arg4, %193 {mhlo.sharding = ""} : tensor<64x10xf32> loc(#loc73)\n    %195 = mhlo.log %36 {mhlo.sharding = "\\08\\03\\1A\\01\\02\\22\\02\\00\\01"} : tensor<100xf32> loc(#loc108)\n    %196 = "mhlo.broadcast_in_dim"(%195) {broadcast_dimensions = dense<0> : tensor<1xi64>, mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc29)\n    %197 = mhlo.subtract %33, %196 {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : tensor<100x10xf32> loc(#loc29)\n    %198 = "mhlo.select"(%9, %197, %13) {mhlo.sharding = "\\08\\03\\1A\\02\\02\\01\\22\\02\\00\\01"} : (tensor<100x10xi1>, tensor<100x10xf32>, tensor<100x10xf32>) -> tensor<100x10xf32> loc(#loc18)\n    %199 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc109)\n    %200 = mhlo.reduce(%198 init: %199) across dimensions = [1] : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc3)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc3)\n    %201 = mhlo.negate %200 {mhlo.sharding = "\\08\\03\\1A\\01\\02\\22\\02\\00\\01"} : tensor<100xf32> loc(#loc19)\n    %202 = mhlo.constant {mhlo.sharding = ""} dense<0.000000e+00> : tensor<f32> loc(#loc110)\n    %203 = mhlo.reduce(%201 init: %202) across dimensions = [0] : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n     reducer(%arg16: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]), %arg17: tensor<f32> loc(fused["parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0]))  {\n      %207 = mhlo.add %arg16, %arg17 : tensor<f32> loc(#loc4)\n      mhlo.return %207 : tensor<f32> loc(#loc0)\n    } loc(#loc4)\n    %204 = mhlo.constant {mhlo.sharding = ""} dense<0.00999999977> : tensor<f32> loc(#loc111)\n    %205 = mhlo.multiply %203, %204 {mhlo.sharding = ""} : tensor<f32> loc(#loc32)\n    %206 = "mhlo.tuple"(%1, %94, %127, %161, %194, %62, %54, %102, %136, %169, %74, %111, %145, %178, %205) {mhlo.sharding = "\\08\\02*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00*\\00", xla_shape = "(s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{1,0}, /*index=5*/s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, /*index=10*/f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, f32[])"} : (tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>) -> tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc112)\n    return %206 : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc0)\n  } loc(#loc0)\n} loc(#loc0)\n#loc6 = loc("constant.5")\n#loc7 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/flax/training/train_state.py":77:0])\n#loc8 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 3) rhs_shape=(3, 3, 3, 32) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc9 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 32) rhs_shape=(3, 3, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc10 = loc("constant.73")\n#loc11 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/max", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc12 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/gt", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc13 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/eq", "/data/czy/alpa/vgg16-alpa.py":123:0])\n#loc14 = loc("constant.77")\n#loc15 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 10) broadcast_dimensions=(0,)]", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc16 = loc("constant.12")\n#loc17 = loc("broadcast")\n#loc18 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc19 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/neg", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc20 = loc("constant.10")\n#loc21 = loc("constant.6")\n#loc22 = loc("constant.1")\n#loc23 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc24 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc25 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/data/czy/neurai/neurai/nn/layer/linear.py":98:0])\n#loc26 = loc("constant.80")\n#loc27 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc28 = loc("constant.87")\n#loc29 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sub", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc30 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/exp", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc31 = loc("constant.7")\n#loc32 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc33 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add_any", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc34 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc35 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 32, 32, 64) broadcast_dimensions=(0, 3)]", "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py":3462:0])\n#loc36 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/select_n", "/data/czy/neurai/neurai/nn/layer/activate.py":25:0])\n#loc37 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/rev[dimensions=(0, 1)]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc38 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(2, 3, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 64) rhs_shape=(3, 3, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc39 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 3) rhs_shape=(100, 32, 32, 32) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc40 = loc("constant.15")\n#loc41 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":84:0])\n#loc42 = loc("constant.20")\n#loc43 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":84:0])\n#loc44 = loc("constant.24")\n#loc45 = loc("constant.22")\n#loc46 = loc("constant.83")\n#loc47 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/lt", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc48 = loc("constant.82")\n#loc49 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc50 = loc("constant.26")\n#loc51 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/select_n", "/usr/local/lib/python3.8/dist-packages/optax/_src/numerics.py":118:0])\n#loc52 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=True]", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc53 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/pow", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc54 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sub", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":118:0])\n#loc55 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":122:0])\n#loc56 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":98:0])\n#loc57 = loc("constant.32")\n#loc58 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":107:0])\n#loc59 = loc("constant.38")\n#loc60 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":107:0])\n#loc61 = loc("constant.79")\n#loc62 = loc("constant.40")\n#loc63 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/sqrt", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc64 = loc("constant.46")\n#loc65 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc66 = loc("multiply.2")\n#loc67 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/div", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":348:0])\n#loc68 = loc("constant.52")\n#loc69 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":770:0])\n#loc70 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":770:0])\n#loc71 = loc("constant.58")\n#loc72 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/mul", "/usr/local/lib/python3.8/dist-packages/optax/_src/transform.py":518:0])\n#loc73 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/add", "/usr/local/lib/python3.8/dist-packages/optax/_src/update.py":43:0])\n#loc74 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(100, 32, 32, 32) rhs_shape=(100, 32, 32, 64) precision=None preferred_element_type=None]", "/data/czy/neurai/neurai/nn/layer/conv.py":212:0])\n#loc75 = loc("constant.14")\n#loc76 = loc("constant.18")\n#loc77 = loc("constant.30")\n#loc78 = loc("constant.36")\n#loc79 = loc("constant.44")\n#loc80 = loc("multiply.3")\n#loc81 = loc("constant.50")\n#loc82 = loc("constant.56")\n#loc83 = loc("constant.11")\n#loc84 = loc("constant.13")\n#loc85 = loc("broadcast.64")\n#loc86 = loc("constant.16")\n#loc87 = loc("broadcast.62")\n#loc88 = loc("constant.28")\n#loc89 = loc("broadcast.48")\n#loc90 = loc("constant.34")\n#loc91 = loc("broadcast.46")\n#loc92 = loc("constant.42")\n#loc93 = loc("broadcast.36")\n#loc94 = loc("multiply.4")\n#loc95 = loc("constant.48")\n#loc96 = loc("broadcast.28")\n#loc97 = loc("constant.54")\n#loc98 = loc("broadcast.20")\n#loc99 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/transpose[permutation=(1, 0)]", "/data/czy/neurai/neurai/nn/layer/linear.py":95:0])\n#loc100 = loc("constant.59")\n#loc101 = loc("constant.57")\n#loc102 = loc("constant.43")\n#loc103 = loc("constant.41")\n#loc104 = loc("constant.33")\n#loc105 = loc("multiply.5")\n#loc106 = loc("constant.25")\n#loc107 = loc("constant.17")\n#loc108 = loc(fused["parallelize(train_step_shard_parallel)/jit(main)/log", "/data/czy/neurai/neurai/nn/layer/loss.py":42:0])\n#loc109 = loc("constant.8")\n#loc110 = loc("constant.9")\n#loc111 = loc("constant.4")\n#loc112 = loc("tuple.267")\n'
# print(sss4)

sss5='#loc = loc(unknown)\n#loc1 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1, 2)]")\n#loc2 = loc("/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py":3502:0)\n#loc3 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(1,)]")\n#loc4 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/loss.py":42:0)\n#loc5 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(1,)]")\n#loc6 = loc("parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0,)]")\n#loc7 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":98:0)\n#loc110 = loc(fused[#loc1, #loc2])\n#loc111 = loc(fused[#loc3, #loc4])\n#loc112 = loc(fused[#loc5, #loc4])\n#loc113 = loc(fused[#loc6, #loc4])\n#loc114 = loc(fused[#loc6, #loc7])\nmodule @train_step_shard_parallel attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {\n  func.func private @region_0.93(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc110)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_1.117(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<f32> loc(#loc111)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_2.128(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_3.140(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_4.146(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc113)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_5.155(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc112)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func private @region_7.170(%arg0: tensor<f32> loc(unknown), %arg1: tensor<f32> loc(unknown)) -> tensor<f32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<f32> loc(#loc114)\n    return %0 : tensor<f32> loc(#loc)\n  } loc(#loc)\n  func.func @main(%arg0: tensor<i32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg1: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg2: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg3: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg4: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg5: tensor<i32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg6: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg7: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg8: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg9: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg10: tensor<3x3x3x32xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg11: tensor<3x3x32x64xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg12: tensor<10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg13: tensor<64x10xf32> {mhlo.sharding = "{replicated}"} loc(unknown), %arg14: tensor<100x32x32x3xf32> {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} loc(unknown), %arg15: tensor<100xi32> {mhlo.sharding = "{devices=[2]0,1}"} loc(unknown)) -> (tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> {mhlo.sharding = "{{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=5*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=10*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}}"}) attributes {xla_entry_computation_result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>]} {\n    %0 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1> : tensor<i32> loc(#loc8)\n    %1 = stablehlo.add %arg0, %0 {mhlo.sharding = "{replicated}"} : tensor<i32> loc(#loc115)\n    %2 = stablehlo.convolution(%arg14, %arg1) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<3x3x3x32xf32>) -> tensor<100x32x32x32xf32> loc(#loc116)\n    %3 = stablehlo.convolution(%2, %arg2) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc116)\n    %4 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc13)\n    %5 = stablehlo.constant dense<0.000000e+00> : tensor<100x32x32x64xf32> loc(#loc117)\n    %6 = stablehlo.compare  GT, %3, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : (tensor<100x32x32x64xf32>, tensor<100x32x32x64xf32>) -> tensor<100x32x32x64xi1> loc(#loc118)\n    %7 = stablehlo.broadcast_in_dim %arg15, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xi32>) -> tensor<100x10xi32> loc(#loc119)\n    %8 = stablehlo.iota dim = 1 : tensor<100x10xi32> loc(#loc119)\n    %9 = stablehlo.compare  EQ, %7, %8 {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x10xi32>, tensor<100x10xi32>) -> tensor<100x10xi1> loc(#loc119)\n    %10 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-0.00999999977> : tensor<f32> loc(#loc19)\n    %11 = stablehlo.constant dense<-0.00999999977> : tensor<100x10xf32> loc(#loc120)\n    %12 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc21)\n    %13 = stablehlo.constant dense<0.000000e+00> : tensor<100x10xf32> loc(#loc22)\n    %14 = stablehlo.select %9, %11, %13 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xi1>, tensor<100x10xf32> loc(#loc121)\n    %15 = stablehlo.negate %14 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc122)\n    %16 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc25)\n    %17 = stablehlo.reduce(%15 init: %16) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %18 = stablehlo.maximum %3, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : tensor<100x32x32x64xf32> loc(#loc117)\n    %19 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc26)\n    %20 = stablehlo.reduce(%18 init: %19) across dimensions = [1, 2] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x32x32x64xf32>, tensor<f32>) -> tensor<100x64xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc1, #loc2]), %arg17: tensor<f32> loc(fused[#loc1, #loc2]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc110)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc110)\n    %21 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.765625E-4> : tensor<f32> loc(#loc27)\n    %22 = stablehlo.constant dense<9.765625E-4> : tensor<100x64xf32> loc(#loc123)\n    %23 = stablehlo.multiply %20, %22 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x64xf32> loc(#loc123)\n    %24 = stablehlo.dot %23, %arg4, precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x64xf32>, tensor<64x10xf32>) -> tensor<100x10xf32> loc(#loc124)\n    %25 = stablehlo.broadcast_in_dim %arg3, dims = [1] {mhlo.sharding = "{replicated}"} : (tensor<10xf32>) -> tensor<100x10xf32> loc(#loc125)\n    %26 = stablehlo.add %24, %25 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc125)\n    %27 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-07> : tensor<f32> loc(#loc31)\n    %28 = stablehlo.constant dense<1.000000e-07> : tensor<100x10xf32> loc(#loc126)\n    %29 = stablehlo.add %26, %28 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc126)\n    %30 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0xFF800000> : tensor<f32> loc(#loc32)\n    %31 = stablehlo.reduce(%29 init: %30) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc3, #loc4]), %arg17: tensor<f32> loc(fused[#loc3, #loc4]))  {\n      %207 = stablehlo.maximum %arg16, %arg17 : tensor<f32> loc(#loc111)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc111)\n    %32 = stablehlo.broadcast_in_dim %31, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc127)\n    %33 = stablehlo.subtract %29, %32 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc127)\n    %34 = stablehlo.exponential %33 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc128)\n    %35 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc35)\n    %36 = stablehlo.reduce(%34 init: %35) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %37 = stablehlo.divide %17, %36 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc129)\n    %38 = stablehlo.broadcast_in_dim %37, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc120)\n    %39 = stablehlo.multiply %38, %34 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc121)\n    %40 = stablehlo.add %14, %39 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc130)\n    %41 = stablehlo.dot_general %40, %arg4, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100x10xf32>, tensor<64x10xf32>) -> tensor<100x64xf32> loc(#loc131)\n    %42 = stablehlo.multiply %41, %22 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x64xf32> loc(#loc123)\n    %43 = stablehlo.broadcast_in_dim %42, dims = [0, 3] {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : (tensor<100x64xf32>) -> tensor<100x32x32x64xf32> loc(#loc132)\n    %44 = stablehlo.select %6, %43, %5 {mhlo.sharding = "{devices=[2,1,1,1]0,1}"} : tensor<100x32x32x64xi1>, tensor<100x32x32x64xf32> loc(#loc133)\n    %45 = stablehlo.reverse %arg2, dims = [0, 1] : tensor<3x3x32x64xf32> loc(#loc134)\n    %46 = stablehlo.convolution(%44, %45) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{devices=[2,1,1,1]0,1}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x64xf32>, tensor<3x3x32x64xf32>) -> tensor<100x32x32x32xf32> loc(#loc135)\n    %47 = stablehlo.convolution(%arg14, %46) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{replicated}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x3xf32>, tensor<100x32x32x32xf32>) -> tensor<3x3x3x32xf32> loc(#loc136)\n    %48 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc43)\n    %49 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x3x32xf32> loc(#loc137)\n    %50 = stablehlo.multiply %47, %49 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc137)\n    %51 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc45)\n    %52 = stablehlo.constant dense<0.899999976> : tensor<3x3x3x32xf32> loc(#loc137)\n    %53 = stablehlo.multiply %arg6, %52 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc137)\n    %54 = stablehlo.add %50, %53 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc138)\n    %55 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e+00> : tensor<f32> loc(#loc46)\n    %56 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc47)\n    %57 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<2147483647> : tensor<i32> loc(#loc48)\n    %58 = stablehlo.compare  LT, %arg5, %57 {mhlo.sharding = "{replicated}"} : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc139)\n    %59 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1> : tensor<i32> loc(#loc51)\n    %60 = stablehlo.add %arg5, %59 {mhlo.sharding = "{replicated}"} : tensor<i32> loc(#loc140)\n    %61 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<2147483647> : tensor<i32> loc(#loc52)\n    %62 = stablehlo.select %58, %60, %61 {mhlo.sharding = "{replicated}"} : tensor<i1>, tensor<i32> loc(#loc141)\n    %63 = stablehlo.convert %62 {mhlo.sharding = "{replicated}"} : (tensor<i32>) -> tensor<f32> loc(#loc142)\n    %64 = stablehlo.power %56, %63 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc143)\n    %65 = stablehlo.subtract %55, %64 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc144)\n    %66 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc145)\n    %67 = stablehlo.multiply %47, %47 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc146)\n    %68 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc58)\n    %69 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x3x32xf32> loc(#loc147)\n    %70 = stablehlo.multiply %67, %69 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc147)\n    %71 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc60)\n    %72 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x3x32xf32> loc(#loc147)\n    %73 = stablehlo.multiply %arg10, %72 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc147)\n    %74 = stablehlo.add %70, %73 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc148)\n    %75 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e+00> : tensor<f32> loc(#loc61)\n    %76 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc62)\n    %77 = stablehlo.power %76, %63 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc143)\n    %78 = stablehlo.subtract %75, %77 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc144)\n    %79 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x3x32xf32> loc(#loc145)\n    %80 = stablehlo.divide %74, %79 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc145)\n    %81 = stablehlo.sqrt %80 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc149)\n    %82 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc65)\n    %83 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x3x32xf32> loc(#loc150)\n    %84 = stablehlo.add %81, %83 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc150)\n    %85 = stablehlo.multiply %66, %84 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc66)\n    %86 = stablehlo.divide %54, %85 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc151)\n    %87 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc67)\n    %88 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x3x32xf32> loc(#loc152)\n    %89 = stablehlo.multiply %arg1, %88 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc152)\n    %90 = stablehlo.add %86, %89 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc153)\n    %91 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc69)\n    %92 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x3x32xf32> loc(#loc154)\n    %93 = stablehlo.multiply %90, %92 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc154)\n    %94 = stablehlo.add %arg1, %93 {mhlo.sharding = "{replicated}"} : tensor<3x3x3x32xf32> loc(#loc155)\n    %95 = stablehlo.convolution(%2, %44) dim_numbers = [f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f], window = {stride = [1, 1], pad = [[1, 1], [1, 1]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, mhlo.sharding = "{replicated}", precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]} : (tensor<100x32x32x32xf32>, tensor<100x32x32x64xf32>) -> tensor<3x3x32x64xf32> loc(#loc136)\n    %96 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc72)\n    %97 = stablehlo.constant dense<1.000000e-01> : tensor<3x3x32x64xf32> loc(#loc137)\n    %98 = stablehlo.multiply %95, %97 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc137)\n    %99 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc73)\n    %100 = stablehlo.constant dense<0.899999976> : tensor<3x3x32x64xf32> loc(#loc137)\n    %101 = stablehlo.multiply %arg7, %100 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc137)\n    %102 = stablehlo.add %98, %101 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc138)\n    %103 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc145)\n    %104 = stablehlo.multiply %95, %95 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc146)\n    %105 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc74)\n    %106 = stablehlo.constant dense<1.000000e-03> : tensor<3x3x32x64xf32> loc(#loc147)\n    %107 = stablehlo.multiply %104, %106 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc147)\n    %108 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc75)\n    %109 = stablehlo.constant dense<9.990000e-01> : tensor<3x3x32x64xf32> loc(#loc147)\n    %110 = stablehlo.multiply %arg11, %109 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc147)\n    %111 = stablehlo.add %107, %110 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc148)\n    %112 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<3x3x32x64xf32> loc(#loc145)\n    %113 = stablehlo.divide %111, %112 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc145)\n    %114 = stablehlo.sqrt %113 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc149)\n    %115 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc76)\n    %116 = stablehlo.constant dense<9.99999993E-9> : tensor<3x3x32x64xf32> loc(#loc150)\n    %117 = stablehlo.add %114, %116 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc150)\n    %118 = stablehlo.multiply %103, %117 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc77)\n    %119 = stablehlo.divide %102, %118 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc151)\n    %120 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc78)\n    %121 = stablehlo.constant dense<5.000000e-04> : tensor<3x3x32x64xf32> loc(#loc152)\n    %122 = stablehlo.multiply %arg2, %121 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc152)\n    %123 = stablehlo.add %119, %122 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc153)\n    %124 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc79)\n    %125 = stablehlo.constant dense<-3.000000e-04> : tensor<3x3x32x64xf32> loc(#loc154)\n    %126 = stablehlo.multiply %123, %125 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc154)\n    %127 = stablehlo.add %arg2, %126 {mhlo.sharding = "{replicated}"} : tensor<3x3x32x64xf32> loc(#loc155)\n    %128 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc80)\n    %129 = stablehlo.reduce(%40 init: %128) across dimensions = [0] {mhlo.sharding = "{replicated}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<10xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc7]), %arg17: tensor<f32> loc(fused[#loc6, #loc7]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc114)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc114)\n    %130 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc81)\n    %131 = stablehlo.constant dense<1.000000e-01> : tensor<10xf32> loc(#loc82)\n    %132 = stablehlo.multiply %129, %131 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc137)\n    %133 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc83)\n    %134 = stablehlo.constant dense<0.899999976> : tensor<10xf32> loc(#loc84)\n    %135 = stablehlo.multiply %arg8, %134 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc137)\n    %136 = stablehlo.add %132, %135 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc138)\n    %137 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<10xf32> loc(#loc145)\n    %138 = stablehlo.multiply %129, %129 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc146)\n    %139 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc85)\n    %140 = stablehlo.constant dense<1.000000e-03> : tensor<10xf32> loc(#loc86)\n    %141 = stablehlo.multiply %138, %140 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc147)\n    %142 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc87)\n    %143 = stablehlo.constant dense<9.990000e-01> : tensor<10xf32> loc(#loc88)\n    %144 = stablehlo.multiply %arg12, %143 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc147)\n    %145 = stablehlo.add %141, %144 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc148)\n    %146 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<10xf32> loc(#loc145)\n    %147 = stablehlo.divide %145, %146 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc145)\n    %148 = stablehlo.sqrt %147 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc149)\n    %149 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc89)\n    %150 = stablehlo.constant dense<9.99999993E-9> : tensor<10xf32> loc(#loc90)\n    %151 = stablehlo.add %148, %150 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc150)\n    %152 = stablehlo.multiply %137, %151 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc91)\n    %153 = stablehlo.divide %136, %152 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc151)\n    %154 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc92)\n    %155 = stablehlo.constant dense<5.000000e-04> : tensor<10xf32> loc(#loc93)\n    %156 = stablehlo.multiply %arg3, %155 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc152)\n    %157 = stablehlo.add %153, %156 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc153)\n    %158 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc94)\n    %159 = stablehlo.constant dense<-3.000000e-04> : tensor<10xf32> loc(#loc95)\n    %160 = stablehlo.multiply %157, %159 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc154)\n    %161 = stablehlo.add %arg3, %160 {mhlo.sharding = "{replicated}"} : tensor<10xf32> loc(#loc155)\n    %162 = stablehlo.dot_general %23, %40, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : (tensor<100x64xf32>, tensor<100x10xf32>) -> tensor<64x10xf32> loc(#loc156)\n    %163 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-01> : tensor<f32> loc(#loc97)\n    %164 = stablehlo.constant dense<1.000000e-01> : tensor<64x10xf32> loc(#loc137)\n    %165 = stablehlo.multiply %162, %164 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc137)\n    %166 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.899999976> : tensor<f32> loc(#loc98)\n    %167 = stablehlo.constant dense<0.899999976> : tensor<64x10xf32> loc(#loc137)\n    %168 = stablehlo.multiply %arg9, %167 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc137)\n    %169 = stablehlo.add %165, %168 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc138)\n    %170 = stablehlo.broadcast_in_dim %65, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc145)\n    %171 = stablehlo.multiply %162, %162 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc146)\n    %172 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<1.000000e-03> : tensor<f32> loc(#loc99)\n    %173 = stablehlo.constant dense<1.000000e-03> : tensor<64x10xf32> loc(#loc147)\n    %174 = stablehlo.multiply %171, %173 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc147)\n    %175 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.990000e-01> : tensor<f32> loc(#loc100)\n    %176 = stablehlo.constant dense<9.990000e-01> : tensor<64x10xf32> loc(#loc147)\n    %177 = stablehlo.multiply %arg13, %176 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc147)\n    %178 = stablehlo.add %174, %177 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc148)\n    %179 = stablehlo.broadcast_in_dim %78, dims = [] {mhlo.sharding = "{replicated}"} : (tensor<f32>) -> tensor<64x10xf32> loc(#loc145)\n    %180 = stablehlo.divide %178, %179 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc145)\n    %181 = stablehlo.sqrt %180 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc149)\n    %182 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<9.99999993E-9> : tensor<f32> loc(#loc101)\n    %183 = stablehlo.constant dense<9.99999993E-9> : tensor<64x10xf32> loc(#loc150)\n    %184 = stablehlo.add %181, %183 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc150)\n    %185 = stablehlo.multiply %170, %184 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc102)\n    %186 = stablehlo.divide %169, %185 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc151)\n    %187 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<5.000000e-04> : tensor<f32> loc(#loc103)\n    %188 = stablehlo.constant dense<5.000000e-04> : tensor<64x10xf32> loc(#loc152)\n    %189 = stablehlo.multiply %arg4, %188 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc152)\n    %190 = stablehlo.add %186, %189 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc153)\n    %191 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<-3.000000e-04> : tensor<f32> loc(#loc104)\n    %192 = stablehlo.constant dense<-3.000000e-04> : tensor<64x10xf32> loc(#loc154)\n    %193 = stablehlo.multiply %190, %192 {mhlo.sharding = "{replicated}", result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[64,10]{0,1}"} : tensor<64x10xf32> loc(#loc154)\n    %194 = stablehlo.add %arg4, %193 {mhlo.sharding = "{replicated}"} : tensor<64x10xf32> loc(#loc155)\n    %195 = stablehlo.log %36 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc157)\n    %196 = stablehlo.broadcast_in_dim %195, dims = [0] {mhlo.sharding = "{devices=[2,1]0,1}"} : (tensor<100xf32>) -> tensor<100x10xf32> loc(#loc127)\n    %197 = stablehlo.subtract %33, %196 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xf32> loc(#loc127)\n    %198 = stablehlo.select %9, %197, %13 {mhlo.sharding = "{devices=[2,1]0,1}"} : tensor<100x10xi1>, tensor<100x10xf32> loc(#loc121)\n    %199 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc106)\n    %200 = stablehlo.reduce(%198 init: %199) across dimensions = [1] {mhlo.sharding = "{devices=[2]0,1}"} : (tensor<100x10xf32>, tensor<f32>) -> tensor<100xf32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc5, #loc4]), %arg17: tensor<f32> loc(fused[#loc5, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc112)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc112)\n    %201 = stablehlo.negate %200 {mhlo.sharding = "{devices=[2]0,1}"} : tensor<100xf32> loc(#loc122)\n    %202 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.000000e+00> : tensor<f32> loc(#loc107)\n    %203 = stablehlo.reduce(%201 init: %202) across dimensions = [0] {mhlo.sharding = "{replicated}"} : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n     reducer(%arg16: tensor<f32> loc(fused[#loc6, #loc4]), %arg17: tensor<f32> loc(fused[#loc6, #loc4]))  {\n      %207 = stablehlo.add %arg16, %arg17 : tensor<f32> loc(#loc113)\n      stablehlo.return %207 : tensor<f32> loc(#loc)\n    } loc(#loc113)\n    %204 = stablehlo.constant {mhlo.sharding = "{replicated}"} dense<0.00999999977> : tensor<f32> loc(#loc108)\n    %205 = stablehlo.multiply %203, %204 {mhlo.sharding = "{replicated}"} : tensor<f32> loc(#loc129)\n    %206 = stablehlo.tuple %1, %94, %127, %161, %194, %62, %54, %102, %136, %169, %74, %111, %145, %178, %205 {mhlo.sharding = "{{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=5*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=10*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}}", result_layout = [dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[1, 0]> : tensor<2xindex>, dense<> : tensor<0xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<[3, 2, 1, 0]> : tensor<4xindex>, dense<0> : tensor<1xindex>, dense<[0, 1]> : tensor<2xindex>, dense<> : tensor<0xindex>], xla_shape = "(s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{1,0}, /*index=5*/s32[], f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, /*index=10*/f32[3,3,3,32]{3,2,1,0}, f32[3,3,32,64]{3,2,1,0}, f32[10]{0}, f32[64,10]{0,1}, f32[])"} : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc109)\n    return %206 : tuple<tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<i32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<3x3x3x32xf32>, tensor<3x3x32x64xf32>, tensor<10xf32>, tensor<64x10xf32>, tensor<f32>> loc(#loc)\n  } loc(#loc)\n} loc(#loc)\n#loc8 = loc("constant.5")\n#loc9 = loc("parallelize(train_step_shard_parallel)/jit(main)/add")\n#loc10 = loc("/usr/local/lib/python3.9/dist-packages/flax/training/train_state.py":77:0)\n#loc11 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc12 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/conv.py":212:0)\n#loc13 = loc("constant.73")\n#loc14 = loc("parallelize(train_step_shard_parallel)/jit(main)/max")\n#loc15 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/activate.py":25:0)\n#loc16 = loc("parallelize(train_step_shard_parallel)/jit(main)/gt")\n#loc17 = loc("parallelize(train_step_shard_parallel)/jit(main)/eq")\n#loc18 = loc("/data/hejing/distri/alpa/vgg16-alpa.py":121:0)\n#loc19 = loc("constant.77")\n#loc20 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 10) broadcast_dimensions=(0,)]")\n#loc21 = loc("constant.12")\n#loc22 = loc("broadcast")\n#loc23 = loc("parallelize(train_step_shard_parallel)/jit(main)/mul")\n#loc24 = loc("parallelize(train_step_shard_parallel)/jit(main)/neg")\n#loc25 = loc("constant.10")\n#loc26 = loc("constant.6")\n#loc27 = loc("constant.1")\n#loc28 = loc("parallelize(train_step_shard_parallel)/jit(main)/div")\n#loc29 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc30 = loc("/usr/local/lib/python3.9/dist-packages/neurai/nn/layer/linear.py":95:0)\n#loc31 = loc("constant.80")\n#loc32 = loc("constant.85")\n#loc33 = loc("parallelize(train_step_shard_parallel)/jit(main)/sub")\n#loc34 = loc("parallelize(train_step_shard_parallel)/jit(main)/exp")\n#loc35 = loc("constant.7")\n#loc36 = loc("parallelize(train_step_shard_parallel)/jit(main)/add_any")\n#loc37 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]")\n#loc38 = loc("parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(100, 32, 32, 64) broadcast_dimensions=(0, 3)]")\n#loc39 = loc("parallelize(train_step_shard_parallel)/jit(main)/select_n")\n#loc40 = loc("parallelize(train_step_shard_parallel)/jit(main)/rev[dimensions=(0, 1)]")\n#loc41 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(2, 3, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc42 = loc("parallelize(train_step_shard_parallel)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]")\n#loc43 = loc("constant.15")\n#loc44 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":84:0)\n#loc45 = loc("constant.20")\n#loc46 = loc("constant.24")\n#loc47 = loc("constant.22")\n#loc48 = loc("constant.83")\n#loc49 = loc("parallelize(train_step_shard_parallel)/jit(main)/lt")\n#loc50 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/numerics.py":118:0)\n#loc51 = loc("constant.82")\n#loc52 = loc("constant.26")\n#loc53 = loc("parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]")\n#loc54 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":118:0)\n#loc55 = loc("parallelize(train_step_shard_parallel)/jit(main)/pow")\n#loc56 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":122:0)\n#loc57 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":98:0)\n#loc58 = loc("constant.32")\n#loc59 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":107:0)\n#loc60 = loc("constant.38")\n#loc61 = loc("constant.79")\n#loc62 = loc("constant.40")\n#loc63 = loc("parallelize(train_step_shard_parallel)/jit(main)/sqrt")\n#loc64 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":348:0)\n#loc65 = loc("constant.46")\n#loc66 = loc("multiply.2")\n#loc67 = loc("constant.52")\n#loc68 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":770:0)\n#loc69 = loc("constant.58")\n#loc70 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/transform.py":518:0)\n#loc71 = loc("/usr/local/lib/python3.9/dist-packages/optax/_src/update.py":43:0)\n#loc72 = loc("constant.14")\n#loc73 = loc("constant.18")\n#loc74 = loc("constant.30")\n#loc75 = loc("constant.36")\n#loc76 = loc("constant.44")\n#loc77 = loc("multiply.3")\n#loc78 = loc("constant.50")\n#loc79 = loc("constant.56")\n#loc80 = loc("constant.11")\n#loc81 = loc("constant.13")\n#loc82 = loc("broadcast.64")\n#loc83 = loc("constant.16")\n#loc84 = loc("broadcast.62")\n#loc85 = loc("constant.28")\n#loc86 = loc("broadcast.48")\n#loc87 = loc("constant.34")\n#loc88 = loc("broadcast.46")\n#loc89 = loc("constant.42")\n#loc90 = loc("broadcast.36")\n#loc91 = loc("multiply.4")\n#loc92 = loc("constant.48")\n#loc93 = loc("broadcast.28")\n#loc94 = loc("constant.54")\n#loc95 = loc("broadcast.20")\n#loc96 = loc("parallelize(train_step_shard_parallel)/jit(main)/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]")\n#loc97 = loc("constant.59")\n#loc98 = loc("constant.57")\n#loc99 = loc("constant.43")\n#loc100 = loc("constant.41")\n#loc101 = loc("constant.33")\n#loc102 = loc("multiply.5")\n#loc103 = loc("constant.25")\n#loc104 = loc("constant.17")\n#loc105 = loc("parallelize(train_step_shard_parallel)/jit(main)/log")\n#loc106 = loc("constant.8")\n#loc107 = loc("constant.9")\n#loc108 = loc("constant.4")\n#loc109 = loc("tuple.267")\n#loc115 = loc(fused[#loc9, #loc10])\n#loc116 = loc(fused[#loc11, #loc12])\n#loc117 = loc(fused[#loc14, #loc15])\n#loc118 = loc(fused[#loc16, #loc15])\n#loc119 = loc(fused[#loc17, #loc18])\n#loc120 = loc(fused[#loc20, #loc4])\n#loc121 = loc(fused[#loc23, #loc4])\n#loc122 = loc(fused[#loc24, #loc4])\n#loc123 = loc(fused[#loc28, #loc2])\n#loc124 = loc(fused[#loc29, #loc30])\n#loc125 = loc(fused[#loc9, #loc7])\n#loc126 = loc(fused[#loc9, #loc4])\n#loc127 = loc(fused[#loc33, #loc4])\n#loc128 = loc(fused[#loc34, #loc4])\n#loc129 = loc(fused[#loc28, #loc4])\n#loc130 = loc(fused[#loc36, #loc4])\n#loc131 = loc(fused[#loc37, #loc30])\n#loc132 = loc(fused[#loc38, #loc2])\n#loc133 = loc(fused[#loc39, #loc15])\n#loc134 = loc(fused[#loc40, #loc12])\n#loc135 = loc(fused[#loc41, #loc12])\n#loc136 = loc(fused[#loc42, #loc12])\n#loc137 = loc(fused[#loc23, #loc44])\n#loc138 = loc(fused[#loc9, #loc44])\n#loc139 = loc(fused[#loc49, #loc50])\n#loc140 = loc(fused[#loc9, #loc50])\n#loc141 = loc(fused[#loc39, #loc50])\n#loc142 = loc(fused[#loc53, #loc54])\n#loc143 = loc(fused[#loc55, #loc54])\n#loc144 = loc(fused[#loc33, #loc54])\n#loc145 = loc(fused[#loc28, #loc56])\n#loc146 = loc(fused[#loc23, #loc57])\n#loc147 = loc(fused[#loc23, #loc59])\n#loc148 = loc(fused[#loc9, #loc59])\n#loc149 = loc(fused[#loc63, #loc64])\n#loc150 = loc(fused[#loc9, #loc64])\n#loc151 = loc(fused[#loc28, #loc64])\n#loc152 = loc(fused[#loc23, #loc68])\n#loc153 = loc(fused[#loc9, #loc68])\n#loc154 = loc(fused[#loc23, #loc70])\n#loc155 = loc(fused[#loc9, #loc71])\n#loc156 = loc(fused[#loc96, #loc30])\n#loc157 = loc(fused[#loc105, #loc4])\n'
print(sss5)
